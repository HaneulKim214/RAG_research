{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31f4edd-0483-4ae0-9405-42a2438b90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random, tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from prompt_templates import QAGenerationPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31e26e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for robust data validation\n",
    "from pydantic import BaseModel, ValidationError, Field, field_validator\n",
    "from typing import Optional, Dict, Any\n",
    "import re\n",
    "\n",
    "class QAPairResponse(BaseModel):\n",
    "    \"\"\"Model for validating QA pair generation response\"\"\"\n",
    "    question: str = Field(..., min_length=10, description=\"The generated question\")\n",
    "    answer: str = Field(..., min_length=20, description=\"The generated answer\")\n",
    "    context: str = Field(..., description=\"The context used for generation\")\n",
    "    generation_successful: bool = Field(default=True, description=\"Whether generation was successful\")\n",
    "    \n",
    "    @field_validator('question')\n",
    "    @classmethod\n",
    "    def validate_question(cls, v: str) -> str:\n",
    "        \"\"\"Ensure question is properly formatted\"\"\"\n",
    "        v = v.strip()\n",
    "        if not v.endswith('?'):\n",
    "            raise ValueError('Question must end with a question mark')\n",
    "        if len(v.split()) < 3:\n",
    "            raise ValueError('Question must contain at least 3 words')\n",
    "        return v\n",
    "    \n",
    "    @field_validator('answer')\n",
    "    @classmethod### **Executive Summary**\n",
    "    def validate_answer(cls, v: str) -> str:\n",
    "        \"\"\"Ensure answer is meaningful\"\"\"\n",
    "        v = v.strip()\n",
    "        if len(v.split()) < 1:\n",
    "            raise ValueError('Answer must contain at least 1 words')\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391f4f22-e275-4c3e-b00a-c8184ac878e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/2025_2Q_LGES_audit_report_summary.txt\", 'r') as f:\n",
    "    lges_2025_2q_summ = f.read()\n",
    "\n",
    "with open(\"data/processed/2025_09_11_LGES_news_summary.txt\", \"r\") as f:\n",
    "    lges_2025_09_01_news_summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "627ef16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_about_lges_text = lges_2025_2q_summ + \"\\n\"+ lges_2025_09_01_news_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec321a42-4ccc-4997-84ae-a895dce34ae4",
   "metadata": {},
   "source": [
    "# Synthetic QA Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c370108-ee7e-4da5-b406-97b8b3722068",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([all_about_lges_text])\n",
    "\n",
    "# docs_processed = []\n",
    "# for doc in langchain_docs:\n",
    "#     docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ebba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 25\n",
      "Sample chunk length: 980\n",
      "Total text length: 16102\n",
      "\n",
      "First chunk preview:\n",
      "### **Executive Summary**\n",
      "\n",
      "LG Energy Solution (LGES) stands as a global titan in the rapidly expanding battery industry, a critical enabler of the electric vehicle (EV) and energy storage system (ESS) revolution. Our long-term conviction in LGES remains strong, driven by its technological leadership, diversified customer base, aggressive capacity expansion, and strategic positioning to benefit from global decarbonization efforts.\n",
      "\n",
      "However, the stock's valuation has historically commanded a premium, reflecting its growth prospects. **For investors looking to initiate or increase positions, our analysis suggests a patient and tactical approach, focusing on specific market conditions and operational catalysts rather than chasing momentum.** We believe optimal entry points will emerge during periods of market consolidation, temporary operational headwinds, or when the market underappreciates LGES's long-term strategic advantages and improving profitability profile.\n",
      "\n",
      "---...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk length: {len(chunks[0].page_content) if chunks else 0}\")\n",
    "print(f\"Total text length: {len(all_about_lges_text)}\")\n",
    "\n",
    "# Display first chunk as example\n",
    "if chunks:\n",
    "    print(f\"\\nFirst chunk preview:\\n{chunks[0].page_content}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095acad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… gemini-2.5-flash initialized\n",
      "âœ… gpt-4o-mini initialized\n"
     ]
    }
   ],
   "source": [
    "gemini_flash = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"âœ… gemini-2.5-flash initialized\")\n",
    "\n",
    "\n",
    "# Initialize OpenAI generation model\n",
    "gpt4o_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"âœ… gpt-4o-mini initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0528dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(llm):\n",
    "    if hasattr(llm, 'model_name'):\n",
    "        return llm.model_name\n",
    "    elif hasattr(llm, 'model'):\n",
    "        return llm.model.split(\"/\")[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported LLM type. Expected OpenAI or Gemini model, got: {type(llm).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "659af154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critique model:  gpt-4o-mini\n",
      "generation model:  gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self, critique_model: str, generation_model: str):\n",
    "        self.critique_llm = critique_model\n",
    "        self.generation_llm = generation_model\n",
    "\n",
    "        print(\"critique model: \", get_model_name(self.critique_llm))\n",
    "        print(\"generation model: \", get_model_name(self.generation_llm))\n",
    "    \n",
    "llm_client = LLMClient(\n",
    "    critique_model=gpt4o_mini,\n",
    "    generation_model=gemini_flash\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61b035c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. SYNTHETIC QA GENERATION PIPELINE ===\n",
    "\n",
    "class SyntheticQAGenerator:\n",
    "    \"\"\"Generate synthetic QA pairs using LLM with quality filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_client = llm_client\n",
    "        self.qa_prompts = QAGenerationPrompts()\n",
    "        \n",
    "    def generate_qa_pair(self, company_name:str, context: str) :\n",
    "        \"\"\"Generate a single QA pair from context using Pydantic validation\"\"\"\n",
    "        chain = self.qa_prompts.BASE_QA_GENERATION | self.llm_client.generation_llm\n",
    "        response = chain.invoke({\"company_name\": company_name, \"context\": context})\n",
    "        \n",
    "        # Pydantic-based parsing for robust validation\n",
    "        response = response.content\n",
    "        question_part = response.split(\"Factoid question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "        answer_part = response.split(\"Answer:\")[1].strip()\n",
    "\n",
    "        # Returns a dict\n",
    "        qa_pair = QAPairResponse(\n",
    "            question=question_part,\n",
    "            answer=answer_part,\n",
    "            context=context\n",
    "        ).model_dump()\n",
    "\n",
    "        return qa_pair\n",
    "\n",
    "    \n",
    "    def critique_qa_pair(self, qa_pair, critique_type):\n",
    "        \"\"\"Apply quality critique to QA pair\"\"\"\n",
    "        if not qa_pair.get(\"generation_successful\", False):\n",
    "            return {\"passed\": False, \"reason\": \"Generation failed\"}\n",
    "            \n",
    "        prompt = self.qa_prompts.RELEVANCE_CRITIQUE[critique_type].format(\n",
    "            question=qa_pair[\"question\"],\n",
    "            answer=qa_pair[\"answer\"],\n",
    "            context=qa_pair.get(\"context\", \"\"),\n",
    "            question_type=qa_pair.get(\"question_type\", \"\")\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.call_llm(prompt, max_tokens=200, temperature=0.3)\n",
    "            \n",
    "            # Parse response\n",
    "            passed = \"PASS\" in response.upper()\n",
    "            reason = \"\"\n",
    "            if \"Reason:\" in response:\n",
    "                reason = response.split(\"Reason:\")[1].strip()\n",
    "            \n",
    "            return {\"passed\": passed, \"reason\": reason, \"critique_type\": critique_type}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"passed\": False, \"reason\": f\"Critique failed: {str(e)}\", \"critique_type\": critique_type}\n",
    "    \n",
    "    def generate_filtered_qa_pairs(self, chunks, n_generations_per_type: int = 5, \n",
    "                                 apply_critiques: bool = True):\n",
    "        \"\"\"Generate QA pairs with quality filtering\"\"\"\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        \n",
    "        for question_type in QUESTION_TYPES.keys():\n",
    "            print(f\"\\nðŸ”„ Generating {question_type} questions...\")\n",
    "            \n",
    "            successful_pairs = []\n",
    "            attempts = 0\n",
    "            max_attempts = n_generations_per_type * 3  # Allow more attempts than needed\n",
    "            \n",
    "            while len(successful_pairs) < n_generations_per_type and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                # Sample random chunk\n",
    "                chunk = random.choice(chunks)\n",
    "                context = chunk.page_content\n",
    "                \n",
    "                # Generate QA pair\n",
    "                qa_pair = self.generate_qa_pair(context, question_type)\n",
    "                \n",
    "                if not qa_pair.get(\"generation_successful\", False):\n",
    "                    print(f\"  âŒ Generation failed: {qa_pair.get('error', 'Unknown error')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Apply critiques if enabled\n",
    "                if apply_critiques:\n",
    "                    critiques_passed = []\n",
    "                    for critique_type in [\"relevance\", \"clarity\", \"complexity\"]:\n",
    "                        critique_result = self.critique_qa_pair(qa_pair, critique_type)\n",
    "                        critiques_passed.append(critique_result[\"passed\"])\n",
    "                        \n",
    "                        if not critique_result[\"passed\"]:\n",
    "                            print(f\"  âš ï¸  Failed {critique_type}: {critique_result['reason']}\")\n",
    "                            break\n",
    "                    \n",
    "                    if not all(critiques_passed):\n",
    "                        continue\n",
    "                \n",
    "                # Add metadata\n",
    "                qa_pair.update({\n",
    "                    \"chunk_id\": chunks.index(chunk),\n",
    "                    \"generation_attempt\": attempts,\n",
    "                    \"critiques_applied\": apply_critiques\n",
    "                })\n",
    "                \n",
    "                successful_pairs.append(qa_pair)\n",
    "                print(f\"  âœ… Generated {question_type} question {len(successful_pairs)}/{n_generations_per_type}\")\n",
    "            \n",
    "            all_qa_pairs.extend(successful_pairs)\n",
    "            print(f\"ðŸ“Š Completed {question_type}: {len(successful_pairs)}/{n_generations_per_type} successful\")\n",
    "        \n",
    "        return all_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ca27da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Synthetic QA generator initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize generator\n",
    "qa_generator = SyntheticQAGenerator(llm_client)\n",
    "print(\"âœ… Synthetic QA generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8458ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [01:49<02:43, 10.92s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# random.sample(chunks, int(len(chunks)*0.80))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample_doc \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(chunks):\n\u001b[0;32m----> 7\u001b[0m     qa_pair \u001b[38;5;241m=\u001b[39m \u001b[43mqa_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_qa_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_doc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     qa_pairs\u001b[38;5;241m.\u001b[39mappend(qa_pair)\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mSyntheticQAGenerator.generate_qa_pair\u001b[0;34m(self, company_name, context)\u001b[0m\n\u001b[1;32m     16\u001b[0m response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     17\u001b[0m question_part \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFactoid question:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 18\u001b[0m answer_part \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Returns a dict\u001b[39;00m\n\u001b[1;32m     21\u001b[0m qa_pair \u001b[38;5;241m=\u001b[39m QAPairResponse(\n\u001b[1;32m     22\u001b[0m     question\u001b[38;5;241m=\u001b[39mquestion_part,\n\u001b[1;32m     23\u001b[0m     answer\u001b[38;5;241m=\u001b[39manswer_part,\n\u001b[1;32m     24\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m     25\u001b[0m )\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "company_name = \"LG Energy Solution\"\n",
    "question_type = \"factual\"\n",
    "\n",
    "qa_pairs = []\n",
    "# random.sample(chunks, int(len(chunks)*0.80))\n",
    "for sample_doc in tqdm.tqdm(chunks):\n",
    "    qa_pair = qa_generator.generate_qa_pair(company_name, sample_doc.page_content)\n",
    "    qa_pairs.append(qa_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b12a26",
   "metadata": {},
   "source": [
    "## Evaluate synthetic QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e07a435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synthetic_qa_pairs(qa_pair, model, critique_type):\n",
    "    if critique_type == \"groundedness\":\n",
    "        chain = QAGenerationPrompts.GROUNDEDNESS_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"],   \n",
    "                                 \"context\": qa_pair[\"context\"]})\n",
    "    elif critique_type == \"relevance\":\n",
    "        chain = QAGenerationPrompts.RELEVANCE_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"]})\n",
    "    elif critique_type == \"pair_quality\":\n",
    "        chain = QAGenerationPrompts.QA_PAIR_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"],\n",
    "                                 \"answer\": qa_pair[\"answer\"],\n",
    "                                 \"context\": qa_pair[\"context\"]})\n",
    "    return response\n",
    "\n",
    "for qa_pair in qa_pairs:\n",
    "    criterions = [\"groundedness\", \"relevance\", \"pair_quality\"]\n",
    "    for criterion in criterions:\n",
    "        response = evaluate_synthetic_qa_pairs(\n",
    "            qa_pair, llm_client.generation_llm, criterion)\n",
    "        response = response.content\n",
    "        score = response.split(\"Total rating: \")[-1].strip()\n",
    "        _eval = response.split(\"Evaluation: \")[-1].split(\"Total rating: \")[0].strip()\n",
    "        qa_pair.update({f\"{criterion}_score\": score,\n",
    "                         f\"{criterion}_eval\": _eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d24fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(qa_pairs)\n",
    "df = df.astype({'groundedness_score': 'int',\n",
    "                'relevance_score': 'int', \n",
    "                'pair_quality_score': 'int'})\n",
    "df['high_quality'] = ((df['groundedness_score'] > 3) \n",
    "                    & (df['relevance_score'] > 3) \n",
    "                    & (df['pair_quality_score'] > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f093964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/synthetic_qa_lges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_parquet(\"data/synthetic_qa_lges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672adf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(df, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9291b963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'context', 'generation_successful', 'groundedness_score', 'groundedness_eval', 'relevance_score', 'relevance_eval', 'pair_quality_score', 'pair_quality_eval', 'high_quality'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337deeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What investment approach is suggested for investors looking to buy LG Energy Solution stock? \n",
      "\n",
      "Answer: For investors aiming to initiate or increase positions in LG Energy Solution, a patient and tactical approach is suggested. This strategy emphasizes focusing on specific market conditions and operational catalysts rather than simply chasing market momentum. \n",
      "\n",
      "Context: ### **Executive Summary**\n",
      "\n",
      "LG Energy Solution (LGES) stands as a global titan in the rapidly expanding battery industry, a critical enabler of the electric vehicle (EV) and energy storage system (ESS) revolution. Our long-term conviction in LGES remains strong, driven by its technological leadership, diversified customer base, aggressive capacity expansion, and strategic positioning to benefit from global decarbonization efforts.\n",
      "\n",
      "However, the stock's valuation has historically commanded a premium, reflecting its growth prospects. **For investors looking to initiate or increase positions, our analysis suggests a patient and tactical approach, focusing on specific market conditions and operational catalysts rather than chasing momentum.** We believe optimal entry points will emerge during periods of market consolidation, temporary operational headwinds, or when the market underappreciates LGES's long-term strategic advantages and improving profitability profile.\n",
      "\n",
      "--- \n",
      "\n",
      "Groundness Score: 5 \n",
      "\n",
      "Relevance Score: 4 \n",
      "\n",
      "Pair Quality Score: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 0\n",
    "\n",
    "print(f\"\"\"\n",
    "Question: {eval_df.iloc[N][\"question\"]} \\n\n",
    "Answer: {eval_df.iloc[N][\"answer\"]} \\n\n",
    "Context: {eval_df.iloc[N][\"context\"]} \\n\n",
    "Groundness Score: {eval_df.iloc[N][\"groundedness_score\"]} \\n\n",
    "Relevance Score: {eval_df.iloc[N][\"relevance_score\"]} \\n\n",
    "Pair Quality Score: {eval_df.iloc[N][\"pair_quality_score\"]}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eee63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a20039a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df['groundedness_score'].sort_values().hist(label='Groundedness Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbb8d31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df['relevance_score'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a3c92a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGiZJREFUeJzt3X1slfX98PFPgfYAG8UHRGFURJ2oIKKiBswmTsBs6OSfDQczBN1iHE4ZixuYqBDm0P28kWUaRI2yLEF0W3CLmw+dDgg+TETY0Dh8vJ2ZDwx1LcLWnbu97j8MzYAWeur3tL3O7/VKmtir17nO9+O3Sd+cc9pTlWVZFgAACfTq7gUAAJVDWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDJ9uvoOW1pa4p133okBAwZEVVVVV989ANAJWZbFzp07Y+jQodGrV/uPS3R5WLzzzjtRV1fX1XcLACTw9ttvx7Bhw9r9epeHxYABAyLik4XV1tYmu26xWIzHH388pkyZEtXV1cmu25NU+ozmy79Kn9F8+VfpM5ZzvsbGxqirq2v9Od6eLg+LPU9/1NbWJg+L/v37R21tbUV+s0RU/ozmy79Kn9F8+VfpM3bFfAd7GYMXbwIAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkunyt00HgLw4Zv7vunsJJSn0zuInZ3XvGjxiAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkSgqL5ubmuP7662PEiBHRr1+/OO6442Lx4sWRZVm51gcA5EifUk6+5ZZbYvny5fHzn/88Ro0aFc8//3zMnj07Bg4cGFdffXW51ggA5ERJYfH000/HxRdfHFOnTo2IiGOOOSbuv//+eO6558qyOAAgX0oKiwkTJsRdd90Vr7zySpxwwgnx5z//OTZs2BBLly5t9zZNTU3R1NTU+nljY2NERBSLxSgWi51c9v72XCvlNXuaSp/RfPlX6TOaL/9KnbHQO19P9Rd6fbLecuxhR69ZlZXwAomWlpa47rrr4ic/+Un07t07mpub46abbooFCxa0e5uFCxfGokWL9ju+atWq6N+/f0fvGgDoRrt3744ZM2ZEQ0ND1NbWtnteSWGxevXquPbaa+N//ud/YtSoUbFly5aYO3duLF26NGbNmtXmbdp6xKKuri527NhxwIWVqlgsRn19fUyePDmqq6uTXbcnqfQZzZd/lT6j+fKv1BlHL3ysC1aVTqFXFovHtZRlDxsbG2PQoEEHDYuSngq59tprY/78+XHJJZdERMQpp5wSb731VixZsqTdsCgUClEoFPY7Xl1dXZZv3HJdtyep9BnNl3+VPqP58q+jMzY1V3XBatIrxx529Hol/brp7t27o1evvW/Su3fvaGlpKeUyAECFKukRi4suuihuuummOProo2PUqFGxefPmWLp0aVx22WXlWh8AkCMlhcXPfvazuP766+M73/lObN++PYYOHRpXXHFF3HDDDeVaHwCQIyWFxYABA2LZsmWxbNmyMi0HAMgz7xUCACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkEzJYfH3v/89vvnNb8bhhx8e/fr1i1NOOSWef/75cqwNAMiZPqWc/NFHH8U555wT5513XjzyyCNxxBFHxKuvvhqHHnpoudYHAORISWFxyy23RF1dXdx3332tx0aMGJF8UQBAPpX0VMhvf/vbGDduXHzta1+LwYMHx2mnnRZ33313udYGAORMSY9YvPHGG7F8+fKYN29eXHfddbFx48a4+uqro6amJmbNmtXmbZqamqKpqan188bGxoiIKBaLUSwWP8XS97bnWimv2dNU+ozmy79Kn9F8+VfqjIXeWTmXk1yh1yfrLccedvSaVVmWdfj/Wk1NTYwbNy6efvrp1mNXX311bNy4MZ555pk2b7Nw4cJYtGjRfsdXrVoV/fv37+hdAwDdaPfu3TFjxoxoaGiI2trads8r6RGLIUOGxMknn7zXsZNOOil+/etft3ubBQsWxLx581o/b2xsjLq6upgyZcoBF1aqYrEY9fX1MXny5Kiurk523Z6k0mc0X/5V+ozmy79SZxy98LEuWFU6hV5ZLB7XUpY93POMw8GUFBbnnHNObNu2ba9jr7zySgwfPrzd2xQKhSgUCvsdr66uLss3brmu25NU+ozmy79Kn9F8+dfRGZuaq7pgNemVYw87er2SXrz5ve99L5599tn48Y9/HK+99lqsWrUq7rrrrpgzZ06nFgkAVJaSwuLMM8+MNWvWxP333x+jR4+OxYsXx7Jly2LmzJnlWh8AkCMlPRUSEXHhhRfGhRdeWI61AAA5571CAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMp8qLG6++eaoqqqKuXPnJloOAJBnnQ6LjRs3xooVK2LMmDEp1wMA5FinwuLjjz+OmTNnxt133x2HHnpo6jUBADnVpzM3mjNnTkydOjUmTZoUP/rRjw54blNTUzQ1NbV+3tjYGBERxWIxisViZ+6+TXuulfKaPU2lz2i+/Kv0Gc2Xf6XOWOidlXM5yRV6fbLecuxhR69ZlWVZSf/XVq9eHTfddFNs3Lgx+vbtGxMnToyxY8fGsmXL2jx/4cKFsWjRov2Or1q1Kvr371/KXQMA3WT37t0xY8aMaGhoiNra2nbPKyks3n777Rg3blzU19e3vrbiYGHR1iMWdXV1sWPHjgMurFTFYjHq6+tj8uTJUV1dney6PUmlz2i+/Kv0Gc2Xf6XOOHrhY12wqnQKvbJYPK6lLHvY2NgYgwYNOmhYlPRUyKZNm2L79u1x+umntx5rbm6O9evXx+233x5NTU3Ru3fvvW5TKBSiUCjsd63q6uqyfOOW67o9SaXPaL78q/QZzZd/HZ2xqbmqC1aTXjn2sKPXKykszj///Ni6detex2bPnh0nnnhi/PCHP9wvKgCA/11KCosBAwbE6NGj9zr2mc98Jg4//PD9jgMA//v4y5sAQDKd+nXT/7Z27doEywAAKoFHLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSKSkslixZEmeeeWYMGDAgBg8eHNOmTYtt27aVa20AQM6UFBbr1q2LOXPmxLPPPhv19fVRLBZjypQpsWvXrnKtDwDIkT6lnPzoo4/u9fnKlStj8ODBsWnTpvjiF7+YdGEAQP6UFBb7amhoiIiIww47rN1zmpqaoqmpqfXzxsbGiIgoFotRLBY/zd3vZc+1Ul6zp6n0Gc2Xf5U+o/nyr9QZC72zci4nuUKvT9Zbjj3s6DWrsizr1P+1lpaW+OpXvxr//Oc/Y8OGDe2et3Dhwli0aNF+x1etWhX9+/fvzF0DAF1s9+7dMWPGjGhoaIja2tp2z+t0WFx55ZXxyCOPxIYNG2LYsGHtntfWIxZ1dXWxY8eOAy6sVMViMerr62Py5MlRXV2d7Lo9SaXPaL78q/QZzZd/pc44euFjXbCqdAq9slg8rqUse9jY2BiDBg06aFh06qmQq666Kh5++OFYv379AaMiIqJQKEShUNjveHV1dVm+cct13Z6k0mc0X/5V+ozmy7+OztjUXNUFq0mvHHvY0euVFBZZlsV3v/vdWLNmTaxduzZGjBjRqcUBAJWppLCYM2dOrFq1Kn7zm9/EgAED4r333ouIiIEDB0a/fv3KskAAID9K+jsWy5cvj4aGhpg4cWIMGTKk9eOBBx4o1/oAgBwp+akQAID2eK8QACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMn06e4FpDZ64WPR1FzV3cvosP9789TuXgIAJOMRCwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAk06mwuOOOO+KYY46Jvn37xtlnnx3PPfdc6nUBADlUclg88MADMW/evLjxxhvjhRdeiFNPPTUuuOCC2L59eznWBwDkSMlhsXTp0vj2t78ds2fPjpNPPjnuvPPO6N+/f9x7773lWB8AkCN9Sjn5P//5T2zatCkWLFjQeqxXr14xadKkeOaZZ9q8TVNTUzQ1NbV+3tDQEBERH374YRSLxc6suU3FYjF2794dfYq9ormlKtl1y+2DDz7o8Ll7Zvzggw+iurq6jKvqHubLv0qf0Xz5V+qMff7fri5YVTp9WrLYvbulLHu4c+fOiIjIsuzAayjlojt27Ijm5uY48sgj9zp+5JFHxl//+tc2b7NkyZJYtGjRfsdHjBhRyl1XrEH/p7tXAEAlmVHm6+/cuTMGDhzY7tdLCovOWLBgQcybN6/185aWlvjwww/j8MMPj6qqdI8sNDY2Rl1dXbz99ttRW1ub7Lo9SaXPaL78q/QZzZd/lT5jOefLsix27twZQ4cOPeB5JYXFoEGDonfv3vH+++/vdfz999+Po446qs3bFAqFKBQKex075JBDSrnbktTW1lbkN8t/q/QZzZd/lT6j+fKv0mcs13wHeqRij5JevFlTUxNnnHFGPPHEE63HWlpa4oknnojx48eXvkIAoKKU/FTIvHnzYtasWTFu3Lg466yzYtmyZbFr166YPXt2OdYHAORIyWExffr0+Mc//hE33HBDvPfeezF27Nh49NFH93tBZ1crFApx44037ve0SyWp9BnNl3+VPqP58q/SZ+wJ81VlB/u9EQCADvJeIQBAMsICAEhGWAAAyQgLACCZ3ITF+vXr46KLLoqhQ4dGVVVVPPTQQwe9zdq1a+P000+PQqEQxx9/fKxcubLs6+ysUudbu3ZtVFVV7ffx3nvvdc2CS7RkyZI488wzY8CAATF48OCYNm1abNu27aC3++Uvfxknnnhi9O3bN0455ZT4/e9/3wWrLV1n5lu5cuV++9e3b98uWnHpli9fHmPGjGn9wzvjx4+PRx555IC3ycv+RZQ+X972b18333xzVFVVxdy5cw94Xp728L91ZL687eHChQv3W++JJ554wNt0x/7lJix27doVp556atxxxx0dOv/NN9+MqVOnxnnnnRdbtmyJuXPnxre+9a147LHHyrzSzil1vj22bdsW7777buvH4MGDy7TCT2fdunUxZ86cePbZZ6O+vj6KxWJMmTIldu1q/w1+nn766fjGN74Rl19+eWzevDmmTZsW06ZNixdffLELV94xnZkv4pO/jvff+/fWW2910YpLN2zYsLj55ptj06ZN8fzzz8eXvvSluPjii+Oll15q8/w87V9E6fNF5Gv//tvGjRtjxYoVMWbMmAOel7c93KOj80Xkbw9HjRq113o3bNjQ7rndtn9ZDkVEtmbNmgOe84Mf/CAbNWrUXsemT5+eXXDBBWVcWRodme+Pf/xjFhHZRx991CVrSm379u1ZRGTr1q1r95yvf/3r2dSpU/c6dvbZZ2dXXHFFuZf3qXVkvvvuuy8bOHBg1y2qDA499NDsnnvuafNred6/PQ40X173b+fOndnnP//5rL6+Pjv33HOza665pt1z87iHpcyXtz288cYbs1NPPbXD53fX/uXmEYtSPfPMMzFp0qS9jl1wwQXtvr17Xo0dOzaGDBkSkydPjqeeeqq7l9NhDQ0NERFx2GGHtXtOnvewI/NFRHz88ccxfPjwqKurO+i/jnuS5ubmWL16dezatavdP+ef5/3ryHwR+dy/OXPmxNSpU/fbm7bkcQ9LmS8if3v46quvxtChQ+PYY4+NmTNnxt/+9rd2z+2u/Sv7u5t2l/fee6/Nt3dvbGyMf/3rX9GvX79uWlkaQ4YMiTvvvDPGjRsXTU1Ncc8998TEiRPjT3/6U5x++undvbwDamlpiblz58Y555wTo0ePbve89vawp76OZI+Ozjdy5Mi49957Y8yYMdHQ0BC33nprTJgwIV566aUYNmxYF66447Zu3Rrjx4+Pf//73/HZz3421qxZEyeffHKb5+Zx/0qZL4/7t3r16njhhRdi48aNHTo/b3tY6nx528Ozzz47Vq5cGSNHjox33303Fi1aFF/4whfixRdfjAEDBux3fnftX8WGRaUbOXJkjBw5svXzCRMmxOuvvx633XZb/OIXv+jGlR3cnDlz4sUXXzzgc4N51tH5xo8fv9e/hidMmBAnnXRSrFixIhYvXlzuZXbKyJEjY8uWLdHQ0BC/+tWvYtasWbFu3bp2f/jmTSnz5W3/3n777bjmmmuivr6+R79AsbM6M1/e9vDLX/5y63+PGTMmzj777Bg+fHg8+OCDcfnll3fjyvZWsWFx1FFHtfn27rW1tbl/tKI9Z511Vo//YX3VVVfFww8/HOvXrz/ovwja28OjjjqqnEv8VEqZb1/V1dVx2mmnxWuvvVam1X16NTU1cfzxx0dExBlnnBEbN26Mn/70p7FixYr9zs3j/pUy3756+v5t2rQptm/fvtcjms3NzbF+/fq4/fbbo6mpKXr37r3XbfK0h52Zb189fQ/3dcghh8QJJ5zQ7nq7a/8q9jUW48eP3+vt3SMi6uvrK/rt3bds2RJDhgzp7mW0KcuyuOqqq2LNmjXx5JNPxogRIw56mzztYWfm21dzc3Ns3bq1x+5hW1paWqKpqanNr+Vp/9pzoPn21dP37/zzz4+tW7fGli1bWj/GjRsXM2fOjC1btrT5QzdPe9iZ+fbV0/dwXx9//HG8/vrr7a632/avrC8NTWjnzp3Z5s2bs82bN2cRkS1dujTbvHlz9tZbb2VZlmXz58/PLr300tbz33jjjax///7Ztddem7388svZHXfckfXu3Tt79NFHu2uEAyp1vttuuy176KGHsldffTXbunVrds0112S9evXK/vCHP3TXCAd05ZVXZgMHDszWrl2bvfvuu60fu3fvbj3n0ksvzebPn9/6+VNPPZX16dMnu/XWW7OXX345u/HGG7Pq6ups69at3THCAXVmvkWLFmWPPfZY9vrrr2ebNm3KLrnkkqxv377ZSy+91B0jHNT8+fOzdevWZW+++Wb2l7/8JZs/f35WVVWVPf7441mW5Xv/sqz0+fK2f23Z97cm8r6H+zrYfHnbw+9///vZ2rVrszfffDN76qmnskmTJmWDBg3Ktm/fnmVZz9m/3ITFnl+v3Pdj1qxZWZZl2axZs7Jzzz13v9uMHTs2q6mpyY499tjsvvvu6/J1d1Sp891yyy3Zcccdl/Xt2zc77LDDsokTJ2ZPPvlk9yy+A9qaLSL22pNzzz23dd49HnzwweyEE07IampqslGjRmW/+93vunbhHdSZ+ebOnZsdffTRWU1NTXbkkUdmX/nKV7IXXnih6xffQZdddlk2fPjwrKamJjviiCOy888/v/WHbpble/+yrPT58rZ/bdn3B2/e93BfB5svb3s4ffr0bMiQIVlNTU32uc99Lps+fXr22muvtX69p+yft00HAJKp2NdYAABdT1gAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAk8/8Bg0ZHZ/pXZiQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df['pair_quality_score'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe4e3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 11)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. LLM-AS-A-JUDGE EVALUATION ===\n",
    "# Following HuggingFace approach for answer quality evaluation\n",
    "\n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    LLM-as-a-judge evaluator following HuggingFace methodology\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_client = llm_client\n",
    "        \n",
    "    # Evaluation prompt based on HuggingFace cookbook\n",
    "    EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "    def evaluate_answer(self, question: str, generated_answer: str, reference_answer: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a generated answer against reference using LLM-as-a-judge\n",
    "        \"\"\"\n",
    "        prompt = self.EVALUATION_PROMPT.format(\n",
    "            instruction=question,\n",
    "            response=generated_answer,\n",
    "            reference_answer=reference_answer\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.call_llm(prompt, max_tokens=300, temperature=0.1)\n",
    "            \n",
    "            # Parse feedback and score\n",
    "            if \"[RESULT]\" in response:\n",
    "                parts = response.split(\"[RESULT]\")\n",
    "                feedback = parts[0].replace(\"Feedback:\", \"\").strip()\n",
    "                score_text = parts[1].strip()\n",
    "                \n",
    "                # Extract numeric score\n",
    "                score = None\n",
    "                for char in score_text:\n",
    "                    if char.isdigit():\n",
    "                        score = int(char)\n",
    "                        break\n",
    "                \n",
    "                if score is None:\n",
    "                    score = 1  # Default to lowest score if parsing fails\n",
    "                    \n",
    "                # Normalize score to 0-1 range\n",
    "                normalized_score = (score - 1) / 4\n",
    "                \n",
    "                return {\n",
    "                    \"feedback\": feedback,\n",
    "                    \"raw_score\": score,\n",
    "                    \"normalized_score\": normalized_score,\n",
    "                    \"evaluation_successful\": True\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"feedback\": \"Failed to parse evaluation\",\n",
    "                    \"raw_score\": 1,\n",
    "                    \"normalized_score\": 0.0,\n",
    "                    \"evaluation_successful\": False\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"feedback\": f\"Evaluation failed: {str(e)}\",\n",
    "                \"raw_score\": 1,\n",
    "                \"normalized_score\": 0.0,\n",
    "                \"evaluation_successful\": False\n",
    "            }\n",
    "\n",
    "# Combined retrieval and generation evaluator\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Complete RAG evaluation combining retrieval metrics and LLM-as-a-judge\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_judge = LLMJudgeEvaluator(llm_client)\n",
    "        \n",
    "    def evaluate_rag_system(self, retrieval_function, generation_function, \n",
    "                          qa_dataset: List[Dict], k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive RAG evaluation\n",
    "        \n",
    "        Args:\n",
    "            retrieval_function: Function that takes (question, k) and returns retrieved chunks\n",
    "            generation_function: Function that takes (question, contexts) and returns answer\n",
    "            qa_dataset: List of QA pairs with ground truth\n",
    "            k: Number of chunks to retrieve\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        overall_metrics = {\n",
    "            'retrieval': {'precision_at_k': [], 'recall_at_k': [], 'mrr': [], 'ndcg_at_k': []},\n",
    "            'generation': {'scores': [], 'normalized_scores': []}\n",
    "        }\n",
    "        \n",
    "        print(f\"ðŸ”„ Evaluating RAG system on {len(qa_dataset)} questions...\")\n",
    "        \n",
    "        for i, qa_item in enumerate(qa_dataset):\n",
    "            question = qa_item['question']\n",
    "            reference_answer = qa_item['answer']\n",
    "            ground_truth_chunk_id = qa_item.get('chunk_id', None)\n",
    "            \n",
    "            try:\n",
    "                # 1. Evaluate retrieval\n",
    "                retrieved_chunks = retrieval_function(question, k=k)\n",
    "                \n",
    "                # For retrieval evaluation, we use the source chunk as ground truth\n",
    "                ground_truth_chunks = [ground_truth_chunk_id] if ground_truth_chunk_id is not None else []\n",
    "                \n",
    "                retrieval_metrics = self.evaluate_retrieval(retrieved_chunks, ground_truth_chunks, k)\n",
    "                \n",
    "                # 2. Generate answer using retrieved contexts\n",
    "                contexts = [chunk.get('content', chunk.get('page_content', '')) for chunk in retrieved_chunks]\n",
    "                generated_answer = generation_function(question, contexts)\n",
    "                \n",
    "                # 3. Evaluate generation quality\n",
    "                generation_eval = self.llm_judge.evaluate_answer(question, generated_answer, reference_answer)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'question_type': qa_item.get('question_type', 'unknown'),\n",
    "                    'reference_answer': reference_answer,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'retrieved_chunks': retrieved_chunks,\n",
    "                    'ground_truth_chunk': ground_truth_chunk_id,\n",
    "                    'retrieval_metrics': retrieval_metrics,\n",
    "                    'generation_evaluation': generation_eval\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                for metric_name, value in retrieval_metrics.items():\n",
    "                    overall_metrics['retrieval'][metric_name].append(value)\n",
    "                \n",
    "                overall_metrics['generation']['scores'].append(generation_eval['raw_score'])\n",
    "                overall_metrics['generation']['normalized_scores'].append(generation_eval['normalized_score'])\n",
    "                \n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"  âœ… Evaluated {i + 1}/{len(qa_dataset)} questions\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error evaluating question {i + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate overall averages\n",
    "        avg_metrics = {\n",
    "            'retrieval': {},\n",
    "            'generation': {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, values in overall_metrics['retrieval'].items():\n",
    "            avg_metrics['retrieval'][metric_name] = np.mean(values) if values else 0.0\n",
    "            \n",
    "        avg_metrics['generation']['avg_score'] = np.mean(overall_metrics['generation']['scores']) if overall_metrics['generation']['scores'] else 0.0\n",
    "        avg_metrics['generation']['avg_normalized_score'] = np.mean(overall_metrics['generation']['normalized_scores']) if overall_metrics['generation']['normalized_scores'] else 0.0\n",
    "        \n",
    "        return {\n",
    "            'overall_metrics': avg_metrics,\n",
    "            'detailed_results': results,\n",
    "            'total_evaluated': len(results)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_retrieval(retrieved_chunks, ground_truth_chunks, k=3):\n",
    "        \"\"\"Retrieval evaluation metrics\"\"\"\n",
    "        if not retrieved_chunks or not ground_truth_chunks:\n",
    "            return {'precision_at_k': 0.0, 'recall_at_k': 0.0, 'mrr': 0.0, 'ndcg_at_k': 0.0}\n",
    "            \n",
    "        retrieved_k = retrieved_chunks[:k]\n",
    "        retrieved_ids = set([chunk.get('chunk_id', chunk.get('id', i)) for i, chunk in enumerate(retrieved_k)])\n",
    "        ground_truth_ids = set(ground_truth_chunks)\n",
    "        \n",
    "        relevant_retrieved = len(retrieved_ids.intersection(ground_truth_ids))\n",
    "        \n",
    "        # Precision@K\n",
    "        precision = relevant_retrieved / min(len(retrieved_k), k)\n",
    "        \n",
    "        # Recall@K  \n",
    "        recall = relevant_retrieved / len(ground_truth_ids)\n",
    "        \n",
    "        # MRR\n",
    "        mrr = 0.0\n",
    "        for rank, chunk in enumerate(retrieved_k, 1):\n",
    "            chunk_id = chunk.get('chunk_id', chunk.get('id', rank-1))\n",
    "            if chunk_id in ground_truth_ids:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "        \n",
    "        # NDCG@K (simplified)\n",
    "        dcg = sum(1.0 / np.log2(rank + 1) for rank, chunk in enumerate(retrieved_k, 1) \n",
    "                 if chunk.get('chunk_id', chunk.get('id', rank-1)) in ground_truth_ids)\n",
    "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, min(len(ground_truth_ids), k) + 1))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision_at_k': precision,\n",
    "            'recall_at_k': recall,\n",
    "            'mrr': mrr,\n",
    "            'ndcg_at_k': ndcg\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "rag_evaluator = RAGEvaluator(llm_client)\n",
    "print(\"âœ… LLM-as-a-judge RAG evaluator initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Precision@K: Fraction of retrieved chunks that are relevant\")\n",
    "print(\"- Recall@K: Fraction of relevant chunks that are retrieved\") \n",
    "print(\"- MRR: Mean Reciprocal Rank of first relevant chunk\")\n",
    "print(\"- NDCG@K: Normalized Discounted Cumulative Gain\")\n",
    "\n",
    "# Example evaluation\n",
    "example_qa = synthetic_qa_dataset[0]\n",
    "print(f\"\\n=== EXAMPLE EVALUATION ===\")\n",
    "print(f\"Question: {example_qa['question']}\")\n",
    "print(f\"Ground truth chunks: {example_qa['ground_truth_chunk_ids']}\")\n",
    "\n",
    "# Simulate retrieval results (in practice, this would come from your retrieval system)\n",
    "simulated_retrieval = [\n",
    "    {'chunk_id': example_qa['ground_truth_chunk_ids'][0]},  # First ground truth chunk\n",
    "    {'chunk_id': 999},  # Irrelevant chunk\n",
    "    {'chunk_id': example_qa['ground_truth_chunk_ids'][1] if len(example_qa['ground_truth_chunk_ids']) > 1 else 998}  # Second ground truth or irrelevant\n",
    "]\n",
    "\n",
    "metrics = RetrievalEvaluator.evaluate_retrieval(\n",
    "    simulated_retrieval, \n",
    "    example_qa['ground_truth_chunk_ids'], \n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nSimulated retrieval results: {[chunk['chunk_id'] for chunk in simulated_retrieval]}\")\n",
    "print(\"Evaluation metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc28abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the synthetic QA dataset\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create dataset metadata\n",
    "dataset_metadata = {\n",
    "    \"name\": \"LGES_Synthetic_QA_Dataset\",\n",
    "    \"description\": \"Synthetic question-answer dataset for evaluating retrieval in LGES RAG pipeline\",\n",
    "    \"created_date\": datetime.now().isoformat(),\n",
    "    \"total_questions\": len(synthetic_qa_dataset),\n",
    "    \"question_types\": type_counts,\n",
    "    \"difficulty_distribution\": difficulty_counts,\n",
    "    \"source_text\": \"all_about_lges_text (2Q audit report summary + news summary)\",\n",
    "    \"chunk_settings\": {\n",
    "        \"chunk_size\": 500,\n",
    "        \"chunk_overlap\": 100,\n",
    "        \"total_chunks\": len(chunks)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare the complete dataset\n",
    "complete_dataset = {\n",
    "    \"metadata\": dataset_metadata,\n",
    "    \"questions\": synthetic_qa_dataset,\n",
    "    \"chunks\": [{\"chunk_id\": i, \"content\": chunk.page_content, \"metadata\": chunk.metadata if hasattr(chunk, 'metadata') else {}} \n",
    "              for i, chunk in enumerate(chunks)]\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"data/synthetic_qa_lges_dataset.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(complete_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Synthetic QA dataset saved to: {output_file}\")\n",
    "print(f\"ðŸ“Š Dataset contains {len(synthetic_qa_dataset)} questions and {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. SAVE SYNTHETIC DATASET AND SETUP FOR EVALUATION ===\n",
    "\n",
    "def save_synthetic_dataset(qa_dataset: List[Dict], chunks: List, filename: str = \"data/synthetic_qa_lges_advanced.json\"):\n",
    "    \"\"\"Save the generated synthetic dataset with metadata\"\"\"\n",
    "    \n",
    "    if not qa_dataset:\n",
    "        print(\"âš ï¸  No QA dataset to save\")\n",
    "        return\n",
    "        \n",
    "    # Create dataset metadata\n",
    "    type_counts = {}\n",
    "    for qa in qa_dataset:\n",
    "        qtype = qa[\"question_type\"]\n",
    "        type_counts[qtype] = type_counts.get(qtype, 0) + 1\n",
    "    \n",
    "    dataset_metadata = {\n",
    "        \"name\": \"LGES_Advanced_Synthetic_QA_Dataset\",\n",
    "        \"description\": \"Advanced synthetic QA dataset generated using LLM with critique filtering\",\n",
    "        \"created_date\": datetime.now().isoformat(),\n",
    "        \"generation_method\": \"LLM-based with critique agents\",\n",
    "        \"llm_provider\": llm_client.provider,\n",
    "        \"llm_model\": llm_client.model_name,\n",
    "        \"total_questions\": len(qa_dataset),\n",
    "        \"question_types\": type_counts,\n",
    "        \"source_text\": \"all_about_lges_text (2Q audit report + news summary)\",\n",
    "        \"chunk_settings\": {\n",
    "            \"chunk_size\": 500,\n",
    "            \"chunk_overlap\": 100,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        },\n",
    "        \"evaluation_ready\": True\n",
    "    }\n",
    "    \n",
    "    # Prepare complete dataset\n",
    "    complete_dataset = {\n",
    "        \"metadata\": dataset_metadata,\n",
    "        \"qa_pairs\": qa_dataset,\n",
    "        \"chunks\": [\n",
    "            {\n",
    "                \"chunk_id\": i, \n",
    "                \"content\": chunk.page_content, \n",
    "                \"metadata\": chunk.metadata if hasattr(chunk, 'metadata') else {}\n",
    "            } \n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(complete_dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Advanced synthetic dataset saved to: {filename}\")\n",
    "    print(f\"ðŸ“Š Contains {len(qa_dataset)} questions across {len(type_counts)} types\")\n",
    "    return filename\n",
    "\n",
    "# Save the dataset\n",
    "if synthetic_qa_dataset:\n",
    "    dataset_file = save_synthetic_dataset(synthetic_qa_dataset, chunks)\n",
    "else:\n",
    "    print(\"âš ï¸  No synthetic dataset generated yet - run the generation cell above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. INTEGRATION GUIDE AND USAGE EXAMPLES ===\n",
    "\n",
    "print(\"ðŸ”— INTEGRATION WITH YOUR RAG SYSTEM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸš€ STEP 1: Define Your RAG Functions\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Example: Load your existing vector store\n",
    "import pickle\n",
    "\n",
    "# Load vector store (adapt to your setup)\n",
    "with open('data/vector_stores/flat/index.pkl', 'rb') as f:\n",
    "    vector_store = pickle.load(f)\n",
    "\n",
    "# Define retrieval function\n",
    "def my_retrieval_function(question, k=3):\n",
    "    '''Retrieve relevant chunks for a question'''\n",
    "    docs = vector_store.similarity_search(question, k=k)\n",
    "    \n",
    "    # Convert to expected format\n",
    "    retrieved_chunks = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        retrieved_chunks.append({\n",
    "            'chunk_id': i,  # Map to actual chunk ID\n",
    "            'content': doc.page_content,\n",
    "            'score': getattr(doc, 'score', 0.0)\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Define generation function\n",
    "def my_generation_function(question, contexts):\n",
    "    '''Generate answer using retrieved contexts'''\n",
    "    # Combine contexts\n",
    "    context_text = \"\\\\n\\\\n\".join(contexts)\n",
    "    \n",
    "    # Generate answer (use your LLM client)\n",
    "    prompt = f\"Question: {question}\\\\n\\\\nContext: {context_text}\\\\n\\\\nAnswer:\"\n",
    "    answer = llm_client.call_llm(prompt, max_tokens=200)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "ðŸš€ STEP 2: Run Complete RAG Evaluation\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Evaluate your complete RAG system\n",
    "results = rag_evaluator.evaluate_rag_system(\n",
    "    retrieval_function=my_retrieval_function,\n",
    "    generation_function=my_generation_function,\n",
    "    qa_dataset=synthetic_qa_dataset,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "ðŸš€ STEP 3: Analyze Results\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"RETRIEVAL METRICS:\")\n",
    "for metric, value in results['overall_metrics']['retrieval'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\\\nGENERATION METRICS:\")\n",
    "print(f\"  Average Score: {results['overall_metrics']['generation']['avg_score']:.2f}/5\")\n",
    "print(f\"  Normalized Score: {results['overall_metrics']['generation']['avg_normalized_score']:.3f}\")\n",
    "\n",
    "# Analyze by question type\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results['detailed_results'])\n",
    "type_performance = df.groupby('question_type').agg({\n",
    "    'generation_evaluation': lambda x: np.mean([eval_result['normalized_score'] for eval_result in x])\n",
    "}).round(3)\n",
    "print(\"\\\\nPerformance by Question Type:\")\n",
    "print(type_performance)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ’¡ TIPS FOR IMPROVEMENT:\")\n",
    "print(\"â”€\" * 25)\n",
    "print(\"1. ðŸ“Š Experiment with different chunk sizes (200, 500, 1000 tokens)\")\n",
    "print(\"2. ðŸ”„ Try different embedding models (OpenAI, sentence-transformers)\")  \n",
    "print(\"3. ðŸŽ¯ Add reranking for better retrieval precision\")\n",
    "print(\"4. ðŸ“ Improve generation prompts based on question types\")\n",
    "print(\"5. ðŸ” Analyze failed cases to identify systematic issues\")\n",
    "\n",
    "print(f\"\\nâœ… Setup complete! Your synthetic QA dataset is ready for evaluation.\")\n",
    "print(f\"ðŸ“ Dataset saved with {len(synthetic_qa_dataset) if synthetic_qa_dataset else 0} questions\")\n",
    "print(f\"ðŸŽ¯ Follow the HuggingFace methodology: {('https://huggingface.co/learn/cookbook/en/rag_evaluation')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86521a2-b6e6-484b-9594-3c9fd0b0773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
