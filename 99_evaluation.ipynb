{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31f4edd-0483-4ae0-9405-42a2438b90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random, tqdm\n",
    "import numpy as np\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from prompt_templates import QAGenerationPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e26e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic models for robust data validation\n",
    "from pydantic import BaseModel, ValidationError, Field, field_validator\n",
    "from typing import Optional, Dict, Any\n",
    "import re\n",
    "\n",
    "class QAPairResponse(BaseModel):\n",
    "    \"\"\"Model for validating QA pair generation response\"\"\"\n",
    "    question: str = Field(..., min_length=10, description=\"The generated question\")\n",
    "    answer: str = Field(..., min_length=20, description=\"The generated answer\")\n",
    "    context: str = Field(..., description=\"The context used for generation\")\n",
    "    generation_successful: bool = Field(default=True, description=\"Whether generation was successful\")\n",
    "    \n",
    "    @field_validator('question')\n",
    "    @classmethod\n",
    "    def validate_question(cls, v: str) -> str:\n",
    "        \"\"\"Ensure question is properly formatted\"\"\"\n",
    "        v = v.strip()\n",
    "        if not v.endswith('?'):\n",
    "            raise ValueError('Question must end with a question mark')\n",
    "        if len(v.split()) < 3:\n",
    "            raise ValueError('Question must contain at least 3 words')\n",
    "        return v\n",
    "    \n",
    "    @field_validator('answer')\n",
    "    @classmethod### **Executive Summary**\n",
    "    def validate_answer(cls, v: str) -> str:\n",
    "        \"\"\"Ensure answer is meaningful\"\"\"\n",
    "        v = v.strip()\n",
    "        if len(v.split()) < 1:\n",
    "            raise ValueError('Answer must contain at least 5 words')\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04390be-f8d0-4d13-a0e7-0cae22e2dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/2025_1Q_LGES_Audit_Report_CONFS_en_fulltext.txt\", 'r') as f:\n",
    "    lges_2025_1q_fulltext = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391f4f22-e275-4c3e-b00a-c8184ac878e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/2025_2Q_LGES_audit_report_summary.txt\", 'r') as f:\n",
    "    lges_2025_2q_summ = f.read()\n",
    "\n",
    "with open(\"data/processed/2025_09_11_LGES_news_summary.txt\", \"r\") as f:\n",
    "    lges_2025_09_01_news_summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627ef16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_about_lges_text = lges_2025_2q_summ + \"\\n\"+ lges_2025_09_01_news_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec321a42-4ccc-4997-84ae-a895dce34ae4",
   "metadata": {},
   "source": [
    "# Synthetic QA Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c370108-ee7e-4da5-b406-97b8b3722068",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([all_about_lges_text])\n",
    "\n",
    "# docs_processed = []\n",
    "# for doc in langchain_docs:\n",
    "#     docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38ebba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 25\n",
      "Sample chunk length: 980\n",
      "Total text length: 16102\n",
      "\n",
      "First chunk preview:\n",
      "### **Executive Summary**\n",
      "\n",
      "LG Energy Solution (LGES) stands as a global titan in the rapidly expanding battery industry, a critical enabler of the electric vehicle (EV) and energy storage system (ESS) revolution. Our long-term conviction in LGES remains strong, driven by its technological leadership, diversified customer base, aggressive capacity expansion, and strategic positioning to benefit from global decarbonization efforts.\n",
      "\n",
      "However, the stock's valuation has historically commanded a premium, reflecting its growth prospects. **For investors looking to initiate or increase positions, our analysis suggests a patient and tactical approach, focusing on specific market conditions and operational catalysts rather than chasing momentum.** We believe optimal entry points will emerge during periods of market consolidation, temporary operational headwinds, or when the market underappreciates LGES's long-term strategic advantages and improving profitability profile.\n",
      "\n",
      "---...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk length: {len(chunks[0].page_content) if chunks else 0}\")\n",
    "print(f\"Total text length: {len(all_about_lges_text)}\")\n",
    "\n",
    "# Display first chunk as example\n",
    "if chunks:\n",
    "    print(f\"\\nFirst chunk preview:\\n{chunks[0].page_content}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "095acad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemini-2.5-flash initialized\n",
      "✅ gpt-4o-mini initialized\n"
     ]
    }
   ],
   "source": [
    "gemini_flash = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "print(\"✅ gemini-2.5-flash initialized\")\n",
    "\n",
    "\n",
    "# Initialize OpenAI generation model\n",
    "gpt4o_mini = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"✅ gpt-4o-mini initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0528dd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(llm):\n",
    "    if hasattr(llm, 'model_name'):\n",
    "        return llm.model_name\n",
    "    elif hasattr(llm, 'model'):\n",
    "        return llm.model.split(\"/\")[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported LLM type. Expected OpenAI or Gemini model, got: {type(llm).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659af154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critique model:  gpt-4o-mini\n",
      "generation model:  gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "class LLMClient:\n",
    "    def __init__(self, critique_model: str, generation_model: str):\n",
    "        self.critique_llm = critique_model\n",
    "        self.generation_llm = generation_model\n",
    "\n",
    "        print(\"critique model: \", get_model_name(self.critique_llm))\n",
    "        print(\"generation model: \", get_model_name(self.generation_llm))\n",
    "    \n",
    "llm_client = LLMClient(\n",
    "    critique_model=gpt4o_mini,\n",
    "    generation_model=gemini_flash\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61b035c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. SYNTHETIC QA GENERATION PIPELINE ===\n",
    "\n",
    "class SyntheticQAGenerator:\n",
    "    \"\"\"Generate synthetic QA pairs using LLM with quality filtering\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_client = llm_client\n",
    "        self.qa_prompts = QAGenerationPrompts()\n",
    "        \n",
    "    def generate_qa_pair(self, company_name:str, context: str, question_type: str) :\n",
    "        \"\"\"Generate a single QA pair from context using Pydantic validation\"\"\"\n",
    "        chain = self.qa_prompts.BASE_QA_GENERATION | self.llm_client.generation_llm\n",
    "        response = chain.invoke({\"company_name\": company_name\n",
    "                                , \"question_type\": question_type\n",
    "                                , \"context\": context})\n",
    "        \n",
    "        # Pydantic-based parsing for robust validation\n",
    "        response = response.content\n",
    "        question_part = content.split(\"Factoid question:\")[-1].split(\"Answer:\")[0].strip()\n",
    "        answer_part = content.split(\"Answer:\")[1].strip()\n",
    "\n",
    "        # Returns a dict\n",
    "        qa_pair = QAPairResponse(\n",
    "            question=question_part,\n",
    "            answer=answer_part,\n",
    "            question_type=question_type,\n",
    "            context=context\n",
    "        ).model_dump()\n",
    "\n",
    "        return qa_pair\n",
    "\n",
    "    \n",
    "    def critique_qa_pair(self, qa_pair, critique_type):\n",
    "        \"\"\"Apply quality critique to QA pair\"\"\"\n",
    "        if not qa_pair.get(\"generation_successful\", False):\n",
    "            return {\"passed\": False, \"reason\": \"Generation failed\"}\n",
    "            \n",
    "        prompt = self.qa_prompts.RELEVANCE_CRITIQUE[critique_type].format(\n",
    "            question=qa_pair[\"question\"],\n",
    "            answer=qa_pair[\"answer\"],\n",
    "            context=qa_pair.get(\"context\", \"\"),\n",
    "            question_type=qa_pair.get(\"question_type\", \"\")\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.call_llm(prompt, max_tokens=200, temperature=0.3)\n",
    "            \n",
    "            # Parse response\n",
    "            passed = \"PASS\" in response.upper()\n",
    "            reason = \"\"\n",
    "            if \"Reason:\" in response:\n",
    "                reason = response.split(\"Reason:\")[1].strip()\n",
    "            \n",
    "            return {\"passed\": passed, \"reason\": reason, \"critique_type\": critique_type}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"passed\": False, \"reason\": f\"Critique failed: {str(e)}\", \"critique_type\": critique_type}\n",
    "    \n",
    "    def generate_filtered_qa_pairs(self, chunks, n_generations_per_type: int = 5, \n",
    "                                 apply_critiques: bool = True):\n",
    "        \"\"\"Generate QA pairs with quality filtering\"\"\"\n",
    "        \n",
    "        all_qa_pairs = []\n",
    "        \n",
    "        for question_type in QUESTION_TYPES.keys():\n",
    "            print(f\"\\n🔄 Generating {question_type} questions...\")\n",
    "            \n",
    "            successful_pairs = []\n",
    "            attempts = 0\n",
    "            max_attempts = n_generations_per_type * 3  # Allow more attempts than needed\n",
    "            \n",
    "            while len(successful_pairs) < n_generations_per_type and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                # Sample random chunk\n",
    "                chunk = random.choice(chunks)\n",
    "                context = chunk.page_content\n",
    "                \n",
    "                # Generate QA pair\n",
    "                qa_pair = self.generate_qa_pair(context, question_type)\n",
    "                \n",
    "                if not qa_pair.get(\"generation_successful\", False):\n",
    "                    print(f\"  ❌ Generation failed: {qa_pair.get('error', 'Unknown error')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Apply critiques if enabled\n",
    "                if apply_critiques:\n",
    "                    critiques_passed = []\n",
    "                    for critique_type in [\"relevance\", \"clarity\", \"complexity\"]:\n",
    "                        critique_result = self.critique_qa_pair(qa_pair, critique_type)\n",
    "                        critiques_passed.append(critique_result[\"passed\"])\n",
    "                        \n",
    "                        if not critique_result[\"passed\"]:\n",
    "                            print(f\"  ⚠️  Failed {critique_type}: {critique_result['reason']}\")\n",
    "                            break\n",
    "                    \n",
    "                    if not all(critiques_passed):\n",
    "                        continue\n",
    "                \n",
    "                # Add metadata\n",
    "                qa_pair.update({\n",
    "                    \"chunk_id\": chunks.index(chunk),\n",
    "                    \"generation_attempt\": attempts,\n",
    "                    \"critiques_applied\": apply_critiques\n",
    "                })\n",
    "                \n",
    "                successful_pairs.append(qa_pair)\n",
    "                print(f\"  ✅ Generated {question_type} question {len(successful_pairs)}/{n_generations_per_type}\")\n",
    "            \n",
    "            all_qa_pairs.extend(successful_pairs)\n",
    "            print(f\"📊 Completed {question_type}: {len(successful_pairs)}/{n_generations_per_type} successful\")\n",
    "        \n",
    "        return all_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ca27da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Synthetic QA generator initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize generator\n",
    "qa_generator = SyntheticQAGenerator(llm_client)\n",
    "print(\"✅ Synthetic QA generator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d145549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factual': 'Direct facts, dates, numbers, or specific information',\n",
       " 'analytical': 'Analysis, interpretation, or synthesis requiring reasoning',\n",
       " 'comparative': 'Comparisons between different aspects, periods, or entities',\n",
       " 'strategic': 'Business strategy, plans, or strategic positioning',\n",
       " 'risk': 'Challenges, risks, or potential issues and uncertainties'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QAGenerationPrompts.QUESTION_TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8458ed83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [00:41<00:03,  3.61s/it]Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 13\n",
      "}\n",
      "].\n",
      "100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    }
   ],
   "source": [
    "company_name = \"LG Energy Solution\"\n",
    "question_type = \"factual\"\n",
    "\n",
    "qa_pairs = []\n",
    "for sample_doc in tqdm.tqdm(random.sample(chunks, int(len(chunks)*0.50))):\n",
    "    qa_pair = qa_generator.generate_qa_pair(company_name, sample_doc.page_content, question_type)\n",
    "    qa_pairs.append(qa_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b12a26",
   "metadata": {},
   "source": [
    "## Evaluate synthetic QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1f8fe1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What raw materials affect gross margins?',\n",
       "  'answer': 'Lithium, nickel, and cobalt.',\n",
       "  'context': '6.  **Operational Execution:** Successfully ramping up numerous new, complex manufacturing facilities globally without significant delays or cost overruns is a continuous challenge.\\n7.  **Product Recalls:** While past issues (e.g., GM Bolt, Hyundai Kona) have largely been resolved, any future large-scale recalls could damage reputation and incur significant costs.',\n",
       "  'generation_successful': True},\n",
       " {'question': 'What raw materials affect gross margins?',\n",
       "  'answer': 'Lithium, nickel, and cobalt.',\n",
       "  'context': '*   **LG에너지솔루션 기술 개발:** KAIST 연구팀과 공동으로 12분 급속 충전 기술을 개발했습니다. 이는 흑연 음극을 통해 리튬 메탈을 대체하는 연구로, 기존 리튬이온전지의 짧은 주행거리와 리튬메탈전지의 급속 충전 불가(내부 단락 발생) 단점을 보완하여 충전 시간과 주행 거리를 획기적으로 개선한 기술입니다. 이로 인해 리튬 사용이 더 늘어날 것이라는 긍정적인 전망이 나옵니다.\\n    *   **LG에너지솔루션 대규모 계약:** 메르세데스-벤츠와 약 15조원 규모의 초대형 계약을 체결했습니다. 이는 전기차 150만 대에서 최대 200만 대까지 적용 가능한 분량으로, 4680 원통형 배터리가 탑재될 예정입니다.\\n    *   **경쟁사 동향:** SK온은 북미 시장에 진출하여 1GWh 확정, 6.2GWh 우선협상 대상(총 7.1GWh, 약 2조원 규모 추산)으로 LFP 파우치형 배터리를 공급할 예정입니다. LG에너지솔루션과 SK온 모두 중국이 강세를 보이는 유럽 시장, ESS, LFP 시장에서 성과를 내고 있어 한국 2차전지 산업의 먹거리가 확대될 것이라는 기대감이 있습니다.\\n    *   **소부장 기업 실적 개선:** 에코앤드림, J-Tech, 필에너지 등 중소형 소부장 기업들도 매출 증가 및 적자폭 축소/흑자 전환 등 실적 개선 흐름을 보이고 있습니다. 이는 2차전지 산업 전반이 저점을 찍고 반등하며, ESS 및 전고체 등 다변화 노력이 실적으로 이어지고 있다는 신호로 해석됩니다.',\n",
       "  'generation_successful': True},\n",
       " {'question': 'What raw materials affect gross margins?',\n",
       "  'answer': 'Lithium, nickel, and cobalt.',\n",
       "  'context': \"*   **Capital Expenditure (CapEx):** Extremely high, indicative of the company's aggressive capacity expansion plans. This is a necessary investment for future growth but will continue to weigh on free cash flow in the near to medium term.\\n*   **Cash Flow:** Operating cash flow is generally positive, but free cash flow often remains negative due to the massive CapEx requirements. The company has managed its balance sheet effectively post-IPO, but debt levels are likely to increase to fund expansion.\\n*   **Guidance:** Management typically provides ambitious revenue targets and outlines significant CapEx plans, while cautiously guiding on profitability, acknowledging the dynamic raw material and competitive landscape.\",\n",
       "  'generation_successful': True}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0e07a435",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 10\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 4\n",
      "}\n",
      "].\n"
     ]
    }
   ],
   "source": [
    "def evaluate_synthetic_qa_pairs(qa_pair, model, critique_type):\n",
    "    if critique_type == \"groundedness\":\n",
    "        chain = QAGenerationPrompts.GROUNDEDNESS_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"],   \n",
    "                                 \"context\": qa_pair[\"context\"]})\n",
    "    elif critique_type == \"relevance\":\n",
    "        chain = QAGenerationPrompts.RELEVANCE_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"]})\n",
    "    elif critique_type == \"pair_quality\":\n",
    "        chain = QAGenerationPrompts.QA_PAIR_CRITIQUE_PROMPT | model\n",
    "        response = chain.invoke({\"question\": qa_pair[\"question\"],\n",
    "                                 \"answer\": qa_pair[\"answer\"],\n",
    "                                 \"context\": qa_pair[\"context\"]})\n",
    "    return response\n",
    "\n",
    "for qa_pair in qa_pairs:\n",
    "    criterions = [\"groundedness\", \"relevance\", \"pair_quality\"]\n",
    "    for criterion in criterions:\n",
    "        response = evaluate_synthetic_qa_pairs(\n",
    "            qa_pair, llm_client.generation_llm, criterion)\n",
    "        response = response.content\n",
    "        score = response.split(\"Total rating: \")[-1].strip()\n",
    "        _eval = response.split(\"Evaluation: \")[-1].split(\"Total rating: \")[0].strip()\n",
    "        qa_pair.update({f\"{criterion}_score\": score,\n",
    "                         f\"{criterion}_eval\": _eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24fc7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(qa_pairs)\n",
    "df = df.astype({'groundedness_score': 'int',\n",
    "                'relevance_score': 'int', \n",
    "                'pair_quality_score': 'int'})\n",
    "df['high_quality'] = ((df['groundedness_score'] > 3) \n",
    "                    & (df['relevance_score'] > 3) \n",
    "                    & (df['pair_quality_score'] > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f093964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"data/synthetic_qa_lges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe8ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/synthetic_qa_lges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a20039a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['groundedness_score'].sort_values().hist(label='Groundedness Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dbb8d31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['relevance_score'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0a3c92a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG4BJREFUeJzt3X9s3VX9+PHX7XZ3odIORjdhrgOEADIcRgZkgghoWSZM5h9EGcaJhBgZKi4SnMmgDc4NSZaRj2SAP8DE1IHo8EccUDDbQnBxm84w/0Ag6BCGYyC92xov99N7v3+Y7us+Xbfe7rx7ueXxSJp4377vuS97T3uf3nvXm6tWq9UAAEigqd4DAABjh7AAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkxo/2DVYqlXj11VejpaUlcrncaN88ADAC1Wo19uzZE1OnTo2mpqGflxj1sHj11Vejvb19tG8WAEjg5ZdfjmnTpg353496WLS0tETEfwZrbW1Ntm65XI4nnngiLr/88sjn88nW5d3NviIr9hZZyHJfFYvFaG9v3/84PpRRD4uBlz9aW1uTh0Vzc3O0trb6ISUZ+4qs2FtkYTT21eHexuDNmwBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIpqaw6O/vj6VLl8Ypp5wSRx99dJx66qlxxx13RLVazWo+AKCB1PRZIXfeeWesXr06fvzjH8eMGTNiy5Ytcd1118XEiRPjq1/9alYzAgANoqaweOaZZ+Kqq66KK664IiIiTj755PjpT38af/jDHzIZDgBoLDWFxUc+8pG4//77469//Wucfvrp8ec//zmefvrpWLly5ZDXKZVKUSqV9l8uFosR8Z9PYCuXyyMce7CBtVKuCfYVWbG3yEKW+2q4a+aqNbxBolKpxLe+9a347ne/G+PGjYv+/v5YtmxZLFmyZMjrdHZ2RldX16Dj3d3d0dzcPNybBgDqqK+vLxYsWBC9vb3R2to65Hk1hcWaNWvilltuibvuuitmzJgR27Zti5tvvjlWrlwZCxcuPOh1DvaMRXt7e+zevfuQg9WqXC5HT09PLN3SFKXKoT8r/p1ke+eceo/AIQzsq46Ojsjn8/UehzHE3iILWe6rYrEYbW1thw2Lml4KueWWW+Kb3/xmfPazn42IiA9+8IPx97//PZYvXz5kWBQKhSgUCoOO5/P5TH6YSpVclPobJyz8QmkMWe1XsLfIQhb7arjr1fTPTfv6+qKp6cCrjBs3LiqVSi3LAABjVE3PWMybNy+WLVsW06dPjxkzZsSf/vSnWLlyZXzxi1/Maj4AoIHUFBb/8z//E0uXLo0bb7wxdu3aFVOnTo0vfelLcdttt2U1HwDQQGoKi5aWlli1alWsWrUqo3EAgEbms0IAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkagqLk08+OXK53KCvRYsWZTUfANBAxtdy8ubNm6O/v3//5e3bt0dHR0dcffXVyQcDABpPTWExefLkAy6vWLEiTj311PjYxz6WdCgAoDHVFBb/7e23346f/OQnsXjx4sjlckOeVyqVolQq7b9cLBYjIqJcLke5XB7pzQ8ysFahqZpszdGQ8ntAegP3j/uJ1OwtspDlvhrumrlqtTqiR+KHH344FixYEDt27IipU6cOeV5nZ2d0dXUNOt7d3R3Nzc0juWkAYJT19fXFggULore3N1pbW4c8b8RhMWfOnJgwYUL8+te/PuR5B3vGor29PXbv3n3IwWpVLpejp6cnlm5pilJl6GdQ3mm2d86p9wgcwsC+6ujoiHw+X+9xGEPsLbKQ5b4qFovR1tZ22LAY0Ushf//73+PJJ5+MX/ziF4c9t1AoRKFQGHQ8n89n8sNUquSi1N84YeEXSmPIar+CvUUWsthXw11vRH/H4oEHHogpU6bEFVdcMZKrAwBjVM1hUalU4oEHHoiFCxfG+PEjfu8nADAG1RwWTz75ZOzYsSO++MUvZjEPANDAan7K4fLLL48Rvt8TABjjfFYIAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQTM1h8corr8TnPve5OP744+Poo4+OD37wg7Fly5YsZgMAGsz4Wk7+17/+FRdeeGFceumlsW7dupg8eXI8//zzcdxxx2U1HwDQQGoKizvvvDPa29vjgQce2H/slFNOST4UANCYagqLX/3qVzFnzpy4+uqrY8OGDfG+970vbrzxxrjhhhuGvE6pVIpSqbT/crFYjIiIcrkc5XJ5hGMPNrBWoamabM3RkPJ7QHoD94/7idTsLbKQ5b4a7pq5arU67Efio446KiIiFi9eHFdffXVs3rw5vva1r8W9994bCxcuPOh1Ojs7o6ura9Dx7u7uaG5uHu5NAwB11NfXFwsWLIje3t5obW0d8ryawmLChAkxa9aseOaZZ/Yf++pXvxqbN2+O3//+9we9zsGesWhvb4/du3cfcrBalcvl6OnpiaVbmqJUySVbN2vbO+fUewQOYWBfdXR0RD6fr/c4jCH2FlnIcl8Vi8Voa2s7bFjU9FLIiSeeGGedddYBxz7wgQ/Ez3/+8yGvUygUolAoDDqez+cz+WEqVXJR6m+csPALpTFktV/B3iILWeyr4a5X0z83vfDCC+O555474Nhf//rXOOmkk2pZBgAYo2oKi69//euxadOm+M53vhMvvPBCdHd3x/333x+LFi3Kaj4AoIHUFBbnnXderF27Nn7605/G2WefHXfccUesWrUqrr322qzmAwAaSE3vsYiIuPLKK+PKK6/MYhYAoMH5rBAAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACCZmsKis7MzcrncAV9nnnlmVrMBAA1mfK1XmDFjRjz55JP/f4HxNS8BAIxRNVfB+PHj44QTTshiFgCgwdUcFs8//3xMnTo1jjrqqJg9e3YsX748pk+fPuT5pVIpSqXS/svFYjEiIsrlcpTL5RGMfHADaxWaqsnWHA0pvwekN3D/uJ9Izd4iC1nuq+GumatWq8N+JF63bl3s3bs3zjjjjNi5c2d0dXXFK6+8Etu3b4+WlpaDXqezszO6uroGHe/u7o7m5ubh3jQAUEd9fX2xYMGC6O3tjdbW1iHPqyks/q+33norTjrppFi5cmVcf/31Bz3nYM9YtLe3x+7duw85WK3K5XL09PTE0i1NUarkkq2bte2dc+o9AocwsK86Ojoin8/XexzGEHuLLGS5r4rFYrS1tR02LI7onZfHHntsnH766fHCCy8MeU6hUIhCoTDoeD6fz+SHqVTJRam/ccLCL5TGkNV+BXuLLGSxr4a73hH9HYu9e/fGiy++GCeeeOKRLAMAjBE1hcU3vvGN2LBhQ/ztb3+LZ555Jj796U/HuHHj4pprrslqPgCggdT0Usg//vGPuOaaa+KNN96IyZMnx0UXXRSbNm2KyZMnZzUfANBAagqLNWvWZDUHADAG+KwQACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgmSMKixUrVkQul4ubb7450TgAQCMbcVhs3rw57rvvvpg5c2bKeQCABjaisNi7d29ce+218f3vfz+OO+641DMBAA1q/EiutGjRorjiiiviE5/4RHz7298+5LmlUilKpdL+y8ViMSIiyuVylMvlkdz8QQ2sVWiqJltzNKT8HpDewP3jfiI1e4ssZLmvhrtmzWGxZs2a+OMf/xibN28e1vnLly+Prq6uQcefeOKJaG5urvXmD+uOWZXka2bpt7/9bb1HYBh6enrqPQJjlL1FFrLYV319fcM6L1etVof9f/FffvnlmDVrVvT09Ox/b8Ull1wSH/rQh2LVqlUHvc7BnrFob2+P3bt3R2tr63Bv+rDK5XL09PTE0i1NUarkkq2bte2dc+o9AocwsK86Ojoin8/XexzGEHuLLGS5r4rFYrS1tUVvb+8hH79resZi69atsWvXrvjwhz+8/1h/f39s3Lgxvve970WpVIpx48YdcJ1CoRCFQmHQWvl8PpMfplIlF6X+xgkLv1AaQ1b7FewtspDFvhruejWFxcc//vF49tlnDzh23XXXxZlnnhm33nrroKgAAN5dagqLlpaWOPvssw849p73vCeOP/74QccBgHcff3kTAEhmRP/c9L+tX78+wRgAwFjgGQsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACCZmsJi9erVMXPmzGhtbY3W1taYPXt2rFu3LqvZAIAGU1NYTJs2LVasWBFbt26NLVu2xGWXXRZXXXVV/OUvf8lqPgCggYyv5eR58+YdcHnZsmWxevXq2LRpU8yYMSPpYABA46kpLP5bf39//OxnP4t9+/bF7NmzhzyvVCpFqVTaf7lYLEZERLlcjnK5PNKbH2RgrUJTNdmaoyHl94D0Bu4f9xOp2VtkIct9Ndw1c9VqtaZH4meffTZmz54d//73v+OYY46J7u7u+OQnPznk+Z2dndHV1TXoeHd3dzQ3N9dy0wBAnfT19cWCBQuit7c3Wltbhzyv5rB4++23Y8eOHdHb2xuPPPJI/OAHP4gNGzbEWWedddDzD/aMRXt7e+zevfuQg9WqXC5HT09PLN3SFKVKLtm6WdveOafeI3AIA/uqo6Mj8vl8vcdhDLG3yEKW+6pYLEZbW9thw6Lml0ImTJgQp512WkREnHvuubF58+a4++6747777jvo+YVCIQqFwqDj+Xw+kx+mUiUXpf7GCQu/UBpDVvsV7C2ykMW+Gu56R/x3LCqVygHPSAAA7141PWOxZMmSmDt3bkyfPj327NkT3d3dsX79+nj88cezmg8AaCA1hcWuXbvi85//fOzcuTMmTpwYM2fOjMcffzw6Ojqymg8AaCA1hcUPf/jDrOYAAMYAnxUCACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkU1NYLF++PM4777xoaWmJKVOmxPz58+O5557LajYAoMHUFBYbNmyIRYsWxaZNm6KnpyfK5XJcfvnlsW/fvqzmAwAayPhaTn7ssccOuPzggw/GlClTYuvWrXHxxRcnHQwAaDw1hcX/1dvbGxERkyZNGvKcUqkUpVJp/+VisRgREeVyOcrl8pHc/AEG1io0VZOtORpSfg9Ib+D+cT+Rmr1FFrLcV8NdM1etVkf0SFypVOJTn/pUvPXWW/H0008PeV5nZ2d0dXUNOt7d3R3Nzc0juWkAYJT19fXFggULore3N1pbW4c8b8Rh8eUvfznWrVsXTz/9dEybNm3I8w72jEV7e3vs3r37kIPVqlwuR09PTyzd0hSlSi7Zulnb3jmn3iNwCAP7qqOjI/L5fL3HYQyxtxrD2Z2P13uEmhSaqnHHrEom+6pYLEZbW9thw2JEL4XcdNNN8Zvf/CY2btx4yKiIiCgUClEoFAYdz+fzmfwwlSq5KPU3Tlj4hdIYstqvYG+9szXS48l/y2JfDXe9msKiWq3GV77ylVi7dm2sX78+TjnllBENBwCMTTWFxaJFi6K7uzt++ctfRktLS7z22msRETFx4sQ4+uijMxkQAGgcNf0di9WrV0dvb29ccsklceKJJ+7/euihh7KaDwBoIDW/FAIAMBSfFQIAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACRTc1hs3Lgx5s2bF1OnTo1cLhePPvpoBmMBAI2o5rDYt29fnHPOOXHPPfdkMQ8A0MDG13qFuXPnxty5c7OYBQBocDWHRa1KpVKUSqX9l4vFYkRElMvlKJfLyW5nYK1CUzXZmqMh5feA9AbuH/cTqdlbjaEwrrEeUwYeA7PYV8NdM1etVkf8XcvlcrF27dqYP3/+kOd0dnZGV1fXoOPd3d3R3Nw80psGAEZRX19fLFiwIHp7e6O1tXXI8zIPi4M9Y9He3h67d+8+5GC1KpfL0dPTE0u3NEWpkku2bta2d86p9wjvCmd3Pj6i6xWaqnHHrErd9pX9MTpGuj+OxJHuLXtjdNRjbxyJgX3V0dER+Xw+6drFYjHa2toOGxaZvxRSKBSiUCgMOp7P55P/j46IKFVyUepvnLDI4nvAYEe6J+q1r+yP0VHP3xkj3Vv2xuhopMeT/5bFY+xw1/N3LACAZGp+xmLv3r3xwgsv7L/80ksvxbZt22LSpEkxffr0pMMBAI2l5rDYsmVLXHrppfsvL168OCIiFi5cGA8++GCywQCAxlNzWFxyySVxBO/3BADGMO+xAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhlRWNxzzz1x8sknx1FHHRUXXHBB/OEPf0g9FwDQgGoOi4ceeigWL14ct99+e/zxj3+Mc845J+bMmRO7du3KYj4AoIHUHBYrV66MG264Ia677ro466yz4t57743m5ub40Y9+lMV8AEADGV/LyW+//XZs3bo1lixZsv9YU1NTfOITn4jf//73B71OqVSKUqm0/3Jvb29ERLz55ptRLpdHMvNBlcvl6Ovri/Hlpuiv5JKtm7U33nij3iO8K4z/330ju16lGn19lbrtK/tjdIx0fxzRbR7h3rI3Rkc99saRGNhXb7zxRuTz+aRr79mzJyIiqtXqoU+s1uCVV16pRkT1mWeeOeD4LbfcUj3//PMPep3bb7+9GhG+fPny5cuXrzHw9fLLLx+yFWp6xmIklixZEosXL95/uVKpxJtvvhnHH3985HLp/h9gsViM9vb2ePnll6O1tTXZury72Vdkxd4iC1nuq2q1Gnv27ImpU6ce8ryawqKtrS3GjRsX//znPw84/s9//jNOOOGEg16nUChEoVA44Nixxx5by83WpLW11Q8pydlXZMXeIgtZ7auJEyce9pya3rw5YcKEOPfcc+Opp57af6xSqcRTTz0Vs2fPrn1CAGBMqfmlkMWLF8fChQtj1qxZcf7558eqVati3759cd1112UxHwDQQGoOi8985jPx+uuvx2233RavvfZafOhDH4rHHnss3vve92Yx37AVCoW4/fbbB73sAkfCviIr9hZZeCfsq1z1sP9uBABgeHxWCACQjLAAAJIRFgBAMsICAEim4cNi48aNMW/evJg6dWrkcrl49NFH6z0SY8Dy5cvjvPPOi5aWlpgyZUrMnz8/nnvuuXqPRYNbvXp1zJw5c/8fL5o9e3asW7eu3mMxBnR2dkYulzvg68wzz6zLLA0fFvv27Ytzzjkn7rnnnnqPwhiyYcOGWLRoUWzatCl6enqiXC7H5ZdfHvv2NdYHEvHOMm3atFixYkVs3bo1tmzZEpdddllcddVV8Ze//KXeozEGzJgxI3bu3Ln/6+mnn67LHGPqn5vmcrlYu3ZtzJ8/v96jMMa8/vrrMWXKlNiwYUNcfPHF9R6HMWTSpElx1113xfXXX1/vUWhgnZ2d8eijj8a2bdvqPUrjP2MBo6G3tzci/vMgACn09/fHmjVrYt++fT4SgSSef/75mDp1arz//e+Pa6+9Nnbs2FGXOTL/dFNodJVKJW6++ea48MIL4+yzz673ODS4Z599NmbPnh3//ve/45hjjom1a9fGWWedVe+xaHAXXHBBPPjgg3HGGWfEzp07o6urKz760Y/G9u3bo6WlZVRnERZwGIsWLYrt27fX7fVKxpYzzjgjtm3bFr29vfHII4/EwoULY8OGDeKCIzJ37tz9/3nmzJlxwQUXxEknnRQPP/zwqL/MJizgEG666ab4zW9+Exs3boxp06bVexzGgAkTJsRpp50WERHnnntubN68Oe6+++6477776jwZY8mxxx4bp59+erzwwgujftveYwEHUa1W46abboq1a9fG7373uzjllFPqPRJjVKVSiVKpVO8xGGP27t0bL774Ypx44omjftsN/4zF3r17Dyiyl156KbZt2xaTJk2K6dOn13EyGtmiRYuiu7s7fvnLX0ZLS0u89tprERExceLEOProo+s8HY1qyZIlMXfu3Jg+fXrs2bMnuru7Y/369fH444/XezQa3De+8Y2YN29enHTSSfHqq6/G7bffHuPGjYtrrrlm1Gdp+H9uun79+rj00ksHHV+4cGE8+OCDoz8QY0Iulzvo8QceeCC+8IUvjO4wjBnXX399PPXUU7Fz586YOHFizJw5M2699dbo6Oio92g0uM9+9rOxcePGeOONN2Ly5Mlx0UUXxbJly+LUU08d9VkaPiwAgHcO77EAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMn8P1bGTTUUsUdhAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['pair_quality_score'].sort_values().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. LLM-AS-A-JUDGE EVALUATION ===\n",
    "# Following HuggingFace approach for answer quality evaluation\n",
    "\n",
    "class LLMJudgeEvaluator:\n",
    "    \"\"\"\n",
    "    LLM-as-a-judge evaluator following HuggingFace methodology\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_client = llm_client\n",
    "        \n",
    "    # Evaluation prompt based on HuggingFace cookbook\n",
    "    EVALUATION_PROMPT = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"\n",
    "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{response}\n",
    "\n",
    "###Reference Answer (Score 5):\n",
    "{reference_answer}\n",
    "\n",
    "###Score Rubrics:\n",
    "[Is the response correct, accurate, and factual based on the reference answer?]\n",
    "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
    "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
    "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
    "Score 4: The response is mostly correct, accurate, and factual.\n",
    "Score 5: The response is completely correct, accurate, and factual.\n",
    "\n",
    "###Feedback:\"\"\"\n",
    "\n",
    "    def evaluate_answer(self, question: str, generated_answer: str, reference_answer: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a generated answer against reference using LLM-as-a-judge\n",
    "        \"\"\"\n",
    "        prompt = self.EVALUATION_PROMPT.format(\n",
    "            instruction=question,\n",
    "            response=generated_answer,\n",
    "            reference_answer=reference_answer\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.call_llm(prompt, max_tokens=300, temperature=0.1)\n",
    "            \n",
    "            # Parse feedback and score\n",
    "            if \"[RESULT]\" in response:\n",
    "                parts = response.split(\"[RESULT]\")\n",
    "                feedback = parts[0].replace(\"Feedback:\", \"\").strip()\n",
    "                score_text = parts[1].strip()\n",
    "                \n",
    "                # Extract numeric score\n",
    "                score = None\n",
    "                for char in score_text:\n",
    "                    if char.isdigit():\n",
    "                        score = int(char)\n",
    "                        break\n",
    "                \n",
    "                if score is None:\n",
    "                    score = 1  # Default to lowest score if parsing fails\n",
    "                    \n",
    "                # Normalize score to 0-1 range\n",
    "                normalized_score = (score - 1) / 4\n",
    "                \n",
    "                return {\n",
    "                    \"feedback\": feedback,\n",
    "                    \"raw_score\": score,\n",
    "                    \"normalized_score\": normalized_score,\n",
    "                    \"evaluation_successful\": True\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"feedback\": \"Failed to parse evaluation\",\n",
    "                    \"raw_score\": 1,\n",
    "                    \"normalized_score\": 0.0,\n",
    "                    \"evaluation_successful\": False\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"feedback\": f\"Evaluation failed: {str(e)}\",\n",
    "                \"raw_score\": 1,\n",
    "                \"normalized_score\": 0.0,\n",
    "                \"evaluation_successful\": False\n",
    "            }\n",
    "\n",
    "# Combined retrieval and generation evaluator\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Complete RAG evaluation combining retrieval metrics and LLM-as-a-judge\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm_judge = LLMJudgeEvaluator(llm_client)\n",
    "        \n",
    "    def evaluate_rag_system(self, retrieval_function, generation_function, \n",
    "                          qa_dataset: List[Dict], k: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive RAG evaluation\n",
    "        \n",
    "        Args:\n",
    "            retrieval_function: Function that takes (question, k) and returns retrieved chunks\n",
    "            generation_function: Function that takes (question, contexts) and returns answer\n",
    "            qa_dataset: List of QA pairs with ground truth\n",
    "            k: Number of chunks to retrieve\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        overall_metrics = {\n",
    "            'retrieval': {'precision_at_k': [], 'recall_at_k': [], 'mrr': [], 'ndcg_at_k': []},\n",
    "            'generation': {'scores': [], 'normalized_scores': []}\n",
    "        }\n",
    "        \n",
    "        print(f\"🔄 Evaluating RAG system on {len(qa_dataset)} questions...\")\n",
    "        \n",
    "        for i, qa_item in enumerate(qa_dataset):\n",
    "            question = qa_item['question']\n",
    "            reference_answer = qa_item['answer']\n",
    "            ground_truth_chunk_id = qa_item.get('chunk_id', None)\n",
    "            \n",
    "            try:\n",
    "                # 1. Evaluate retrieval\n",
    "                retrieved_chunks = retrieval_function(question, k=k)\n",
    "                \n",
    "                # For retrieval evaluation, we use the source chunk as ground truth\n",
    "                ground_truth_chunks = [ground_truth_chunk_id] if ground_truth_chunk_id is not None else []\n",
    "                \n",
    "                retrieval_metrics = self.evaluate_retrieval(retrieved_chunks, ground_truth_chunks, k)\n",
    "                \n",
    "                # 2. Generate answer using retrieved contexts\n",
    "                contexts = [chunk.get('content', chunk.get('page_content', '')) for chunk in retrieved_chunks]\n",
    "                generated_answer = generation_function(question, contexts)\n",
    "                \n",
    "                # 3. Evaluate generation quality\n",
    "                generation_eval = self.llm_judge.evaluate_answer(question, generated_answer, reference_answer)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'question': question,\n",
    "                    'question_type': qa_item.get('question_type', 'unknown'),\n",
    "                    'reference_answer': reference_answer,\n",
    "                    'generated_answer': generated_answer,\n",
    "                    'retrieved_chunks': retrieved_chunks,\n",
    "                    'ground_truth_chunk': ground_truth_chunk_id,\n",
    "                    'retrieval_metrics': retrieval_metrics,\n",
    "                    'generation_evaluation': generation_eval\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                for metric_name, value in retrieval_metrics.items():\n",
    "                    overall_metrics['retrieval'][metric_name].append(value)\n",
    "                \n",
    "                overall_metrics['generation']['scores'].append(generation_eval['raw_score'])\n",
    "                overall_metrics['generation']['normalized_scores'].append(generation_eval['normalized_score'])\n",
    "                \n",
    "                if (i + 1) % 5 == 0:\n",
    "                    print(f\"  ✅ Evaluated {i + 1}/{len(qa_dataset)} questions\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error evaluating question {i + 1}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate overall averages\n",
    "        avg_metrics = {\n",
    "            'retrieval': {},\n",
    "            'generation': {}\n",
    "        }\n",
    "        \n",
    "        for metric_name, values in overall_metrics['retrieval'].items():\n",
    "            avg_metrics['retrieval'][metric_name] = np.mean(values) if values else 0.0\n",
    "            \n",
    "        avg_metrics['generation']['avg_score'] = np.mean(overall_metrics['generation']['scores']) if overall_metrics['generation']['scores'] else 0.0\n",
    "        avg_metrics['generation']['avg_normalized_score'] = np.mean(overall_metrics['generation']['normalized_scores']) if overall_metrics['generation']['normalized_scores'] else 0.0\n",
    "        \n",
    "        return {\n",
    "            'overall_metrics': avg_metrics,\n",
    "            'detailed_results': results,\n",
    "            'total_evaluated': len(results)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_retrieval(retrieved_chunks, ground_truth_chunks, k=3):\n",
    "        \"\"\"Retrieval evaluation metrics\"\"\"\n",
    "        if not retrieved_chunks or not ground_truth_chunks:\n",
    "            return {'precision_at_k': 0.0, 'recall_at_k': 0.0, 'mrr': 0.0, 'ndcg_at_k': 0.0}\n",
    "            \n",
    "        retrieved_k = retrieved_chunks[:k]\n",
    "        retrieved_ids = set([chunk.get('chunk_id', chunk.get('id', i)) for i, chunk in enumerate(retrieved_k)])\n",
    "        ground_truth_ids = set(ground_truth_chunks)\n",
    "        \n",
    "        relevant_retrieved = len(retrieved_ids.intersection(ground_truth_ids))\n",
    "        \n",
    "        # Precision@K\n",
    "        precision = relevant_retrieved / min(len(retrieved_k), k)\n",
    "        \n",
    "        # Recall@K  \n",
    "        recall = relevant_retrieved / len(ground_truth_ids)\n",
    "        \n",
    "        # MRR\n",
    "        mrr = 0.0\n",
    "        for rank, chunk in enumerate(retrieved_k, 1):\n",
    "            chunk_id = chunk.get('chunk_id', chunk.get('id', rank-1))\n",
    "            if chunk_id in ground_truth_ids:\n",
    "                mrr = 1.0 / rank\n",
    "                break\n",
    "        \n",
    "        # NDCG@K (simplified)\n",
    "        dcg = sum(1.0 / np.log2(rank + 1) for rank, chunk in enumerate(retrieved_k, 1) \n",
    "                 if chunk.get('chunk_id', chunk.get('id', rank-1)) in ground_truth_ids)\n",
    "        idcg = sum(1.0 / np.log2(rank + 1) for rank in range(1, min(len(ground_truth_ids), k) + 1))\n",
    "        ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision_at_k': precision,\n",
    "            'recall_at_k': recall,\n",
    "            'mrr': mrr,\n",
    "            'ndcg_at_k': ndcg\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "rag_evaluator = RAGEvaluator(llm_client)\n",
    "print(\"✅ LLM-as-a-judge RAG evaluator initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- Precision@K: Fraction of retrieved chunks that are relevant\")\n",
    "print(\"- Recall@K: Fraction of relevant chunks that are retrieved\") \n",
    "print(\"- MRR: Mean Reciprocal Rank of first relevant chunk\")\n",
    "print(\"- NDCG@K: Normalized Discounted Cumulative Gain\")\n",
    "\n",
    "# Example evaluation\n",
    "example_qa = synthetic_qa_dataset[0]\n",
    "print(f\"\\n=== EXAMPLE EVALUATION ===\")\n",
    "print(f\"Question: {example_qa['question']}\")\n",
    "print(f\"Ground truth chunks: {example_qa['ground_truth_chunk_ids']}\")\n",
    "\n",
    "# Simulate retrieval results (in practice, this would come from your retrieval system)\n",
    "simulated_retrieval = [\n",
    "    {'chunk_id': example_qa['ground_truth_chunk_ids'][0]},  # First ground truth chunk\n",
    "    {'chunk_id': 999},  # Irrelevant chunk\n",
    "    {'chunk_id': example_qa['ground_truth_chunk_ids'][1] if len(example_qa['ground_truth_chunk_ids']) > 1 else 998}  # Second ground truth or irrelevant\n",
    "]\n",
    "\n",
    "metrics = RetrievalEvaluator.evaluate_retrieval(\n",
    "    simulated_retrieval, \n",
    "    example_qa['ground_truth_chunk_ids'], \n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(f\"\\nSimulated retrieval results: {[chunk['chunk_id'] for chunk in simulated_retrieval]}\")\n",
    "print(\"Evaluation metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"  {metric_name}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc28abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the synthetic QA dataset\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create dataset metadata\n",
    "dataset_metadata = {\n",
    "    \"name\": \"LGES_Synthetic_QA_Dataset\",\n",
    "    \"description\": \"Synthetic question-answer dataset for evaluating retrieval in LGES RAG pipeline\",\n",
    "    \"created_date\": datetime.now().isoformat(),\n",
    "    \"total_questions\": len(synthetic_qa_dataset),\n",
    "    \"question_types\": type_counts,\n",
    "    \"difficulty_distribution\": difficulty_counts,\n",
    "    \"source_text\": \"all_about_lges_text (2Q audit report summary + news summary)\",\n",
    "    \"chunk_settings\": {\n",
    "        \"chunk_size\": 500,\n",
    "        \"chunk_overlap\": 100,\n",
    "        \"total_chunks\": len(chunks)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare the complete dataset\n",
    "complete_dataset = {\n",
    "    \"metadata\": dataset_metadata,\n",
    "    \"questions\": synthetic_qa_dataset,\n",
    "    \"chunks\": [{\"chunk_id\": i, \"content\": chunk.page_content, \"metadata\": chunk.metadata if hasattr(chunk, 'metadata') else {}} \n",
    "              for i, chunk in enumerate(chunks)]\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"data/synthetic_qa_lges_dataset.json\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(complete_dataset, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Synthetic QA dataset saved to: {output_file}\")\n",
    "print(f\"📊 Dataset contains {len(synthetic_qa_dataset)} questions and {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d03d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. SAVE SYNTHETIC DATASET AND SETUP FOR EVALUATION ===\n",
    "\n",
    "def save_synthetic_dataset(qa_dataset: List[Dict], chunks: List, filename: str = \"data/synthetic_qa_lges_advanced.json\"):\n",
    "    \"\"\"Save the generated synthetic dataset with metadata\"\"\"\n",
    "    \n",
    "    if not qa_dataset:\n",
    "        print(\"⚠️  No QA dataset to save\")\n",
    "        return\n",
    "        \n",
    "    # Create dataset metadata\n",
    "    type_counts = {}\n",
    "    for qa in qa_dataset:\n",
    "        qtype = qa[\"question_type\"]\n",
    "        type_counts[qtype] = type_counts.get(qtype, 0) + 1\n",
    "    \n",
    "    dataset_metadata = {\n",
    "        \"name\": \"LGES_Advanced_Synthetic_QA_Dataset\",\n",
    "        \"description\": \"Advanced synthetic QA dataset generated using LLM with critique filtering\",\n",
    "        \"created_date\": datetime.now().isoformat(),\n",
    "        \"generation_method\": \"LLM-based with critique agents\",\n",
    "        \"llm_provider\": llm_client.provider,\n",
    "        \"llm_model\": llm_client.model_name,\n",
    "        \"total_questions\": len(qa_dataset),\n",
    "        \"question_types\": type_counts,\n",
    "        \"source_text\": \"all_about_lges_text (2Q audit report + news summary)\",\n",
    "        \"chunk_settings\": {\n",
    "            \"chunk_size\": 500,\n",
    "            \"chunk_overlap\": 100,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        },\n",
    "        \"evaluation_ready\": True\n",
    "    }\n",
    "    \n",
    "    # Prepare complete dataset\n",
    "    complete_dataset = {\n",
    "        \"metadata\": dataset_metadata,\n",
    "        \"qa_pairs\": qa_dataset,\n",
    "        \"chunks\": [\n",
    "            {\n",
    "                \"chunk_id\": i, \n",
    "                \"content\": chunk.page_content, \n",
    "                \"metadata\": chunk.metadata if hasattr(chunk, 'metadata') else {}\n",
    "            } \n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(complete_dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Advanced synthetic dataset saved to: {filename}\")\n",
    "    print(f\"📊 Contains {len(qa_dataset)} questions across {len(type_counts)} types\")\n",
    "    return filename\n",
    "\n",
    "# Save the dataset\n",
    "if synthetic_qa_dataset:\n",
    "    dataset_file = save_synthetic_dataset(synthetic_qa_dataset, chunks)\n",
    "else:\n",
    "    print(\"⚠️  No synthetic dataset generated yet - run the generation cell above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. INTEGRATION GUIDE AND USAGE EXAMPLES ===\n",
    "\n",
    "print(\"🔗 INTEGRATION WITH YOUR RAG SYSTEM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "🚀 STEP 1: Define Your RAG Functions\n",
    "────────────────────────────────────\n",
    "\n",
    "# Example: Load your existing vector store\n",
    "import pickle\n",
    "\n",
    "# Load vector store (adapt to your setup)\n",
    "with open('data/vector_stores/flat/index.pkl', 'rb') as f:\n",
    "    vector_store = pickle.load(f)\n",
    "\n",
    "# Define retrieval function\n",
    "def my_retrieval_function(question, k=3):\n",
    "    '''Retrieve relevant chunks for a question'''\n",
    "    docs = vector_store.similarity_search(question, k=k)\n",
    "    \n",
    "    # Convert to expected format\n",
    "    retrieved_chunks = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        retrieved_chunks.append({\n",
    "            'chunk_id': i,  # Map to actual chunk ID\n",
    "            'content': doc.page_content,\n",
    "            'score': getattr(doc, 'score', 0.0)\n",
    "        })\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Define generation function\n",
    "def my_generation_function(question, contexts):\n",
    "    '''Generate answer using retrieved contexts'''\n",
    "    # Combine contexts\n",
    "    context_text = \"\\\\n\\\\n\".join(contexts)\n",
    "    \n",
    "    # Generate answer (use your LLM client)\n",
    "    prompt = f\"Question: {question}\\\\n\\\\nContext: {context_text}\\\\n\\\\nAnswer:\"\n",
    "    answer = llm_client.call_llm(prompt, max_tokens=200)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "🚀 STEP 2: Run Complete RAG Evaluation\n",
    "────────────────────────────────────\n",
    "\n",
    "# Evaluate your complete RAG system\n",
    "results = rag_evaluator.evaluate_rag_system(\n",
    "    retrieval_function=my_retrieval_function,\n",
    "    generation_function=my_generation_function,\n",
    "    qa_dataset=synthetic_qa_dataset,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "🚀 STEP 3: Analyze Results\n",
    "──────────────────────────\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"RETRIEVAL METRICS:\")\n",
    "for metric, value in results['overall_metrics']['retrieval'].items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\\\nGENERATION METRICS:\")\n",
    "print(f\"  Average Score: {results['overall_metrics']['generation']['avg_score']:.2f}/5\")\n",
    "print(f\"  Normalized Score: {results['overall_metrics']['generation']['avg_normalized_score']:.3f}\")\n",
    "\n",
    "# Analyze by question type\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results['detailed_results'])\n",
    "type_performance = df.groupby('question_type').agg({\n",
    "    'generation_evaluation': lambda x: np.mean([eval_result['normalized_score'] for eval_result in x])\n",
    "}).round(3)\n",
    "print(\"\\\\nPerformance by Question Type:\")\n",
    "print(type_performance)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n💡 TIPS FOR IMPROVEMENT:\")\n",
    "print(\"─\" * 25)\n",
    "print(\"1. 📊 Experiment with different chunk sizes (200, 500, 1000 tokens)\")\n",
    "print(\"2. 🔄 Try different embedding models (OpenAI, sentence-transformers)\")  \n",
    "print(\"3. 🎯 Add reranking for better retrieval precision\")\n",
    "print(\"4. 📝 Improve generation prompts based on question types\")\n",
    "print(\"5. 🔍 Analyze failed cases to identify systematic issues\")\n",
    "\n",
    "print(f\"\\n✅ Setup complete! Your synthetic QA dataset is ready for evaluation.\")\n",
    "print(f\"📁 Dataset saved with {len(synthetic_qa_dataset) if synthetic_qa_dataset else 0} questions\")\n",
    "print(f\"🎯 Follow the HuggingFace methodology: {('https://huggingface.co/learn/cookbook/en/rag_evaluation')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c86521a2-b6e6-484b-9594-3c9fd0b0773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
