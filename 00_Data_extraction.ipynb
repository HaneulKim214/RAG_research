{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3528fc5-cdd6-4b33-ab3b-c57e16c68202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, re, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "import fitz\n",
    "import PyPDF2\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9182c975-bfa4-48dd-8b9b-fec5312ef04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e18d56-a9dc-4a69-91ec-e79e2c9f695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43763896-eb3e-4459-a529-545d1bc63dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "            temperature=0.1\n",
    "        )\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c49464c7-a28f-42f1-8f43-f3126f17915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:53:34) [Clang 16.0.6 ]\n",
      "/Users/haneulkim/Desktop/Projects/RAG_experiments/venv_rag/bin/python\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b135fd84-6b9d-4320-8adc-b847bddca20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LG_Energy_Soltuion_Valueup_Plan_ENG.pdf',\n",
       " '252Q_LGES_Audit_Report_CONFS_en.pdf',\n",
       " '.ipynb_checkpoints',\n",
       " '2025_1Q_LGES_Audit_Report_CONFS_en.pdf',\n",
       " 'processed']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bcb0c8-145d-4fb2-9872-f32eab7b2ef2",
   "metadata": {},
   "source": [
    "# Extract data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98323b4-9079-491c-98cd-b038ef1d1a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total pages: 65\n"
     ]
    }
   ],
   "source": [
    "q1_report_path = \"data/2025_1Q_LGES_Audit_Report_CONFS_en.pdf\"\n",
    "docs = fitz.open(q1_report_path)\n",
    "print(f\"total pages: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ab8280c-bd18-43f4-a8d7-fc863715f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document('data/2025_1Q_LGES_Audit_Report_CONFS_en.pdf') <class 'pymupdf.Document'>\n",
      "page 0 of data/2025_1Q_LGES_Audit_Report_CONFS_en.pdf <class 'pymupdf.Page'>\n"
     ]
    }
   ],
   "source": [
    "print(docs, type(docs))\n",
    "print(docs[0], type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed64840d-3b07-4f43-939b-2f3bdb8fbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_scanned_page(page):\n",
    "    \"\"\"\n",
    "    If pages are covered with only images, run OCR.\n",
    "\n",
    "    ??? What if half is filled with images and half with text?\n",
    "    \"\"\"\n",
    "    text = page.get_text().strip()\n",
    "    images = page.get_images()\n",
    "    return len(images) > 0 and len(text) < 25 \n",
    "\n",
    "def extract_text_ocr(page):\n",
    "    pix = page.get_pixmap()\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # text extraction using OCR\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted text\"\"\"\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep punctuation\n",
    "    text = re.sub(r'[^\\w\\s\\-.,;:!?()]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d758d07-5eac-4d2d-aa30-50ec2cbbbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = \"\"\n",
    "for pg_i, page in enumerate(docs):\n",
    "    if is_scanned_page(page):\n",
    "        ocr_text = extract_text_ocr(page)\n",
    "        if ocr_text.strip():\n",
    "            page_text = ocr_text\n",
    "        else:\n",
    "            print(f\"--------OCR failed------- at page:{pg_i}\")\n",
    "    else:\n",
    "        page_text = page.get_text()\n",
    "        \n",
    "    page_text = clean_text(page_text)\n",
    "    full_text += f\"\\n--- Page {pg_i + 1} ---\\n{page_text}\\n\"\n",
    "    \n",
    "with open('data/processed/2025_1Q_LGES_Audit_Report_CONFS_en_fulltext.txt', 'w') as f:\n",
    "   f.write(full_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb25ef3-046e-4678-9866-aeefe8635e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? Extract Table of content: So it's information can be leveraged somewher(ex: chunk by chapter, etc..)\n",
    "# table_of_contents = doc[1].get_text()\n",
    "\n",
    "# ??? Better way to extract table? and standardize schema across various reports from diff countries.\n",
    "# table = doc[5].get_text() + \"\\n\" + doc[6].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d33f6e-afbd-4191-a86c-98920e4a5b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
